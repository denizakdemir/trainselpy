{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainSelPy: Comprehensive Guide\n",
    "\n",
    "This notebook is a comprehensive guide to TrainSelPy, demonstrating all of its key features and applications. TrainSelPy is a Python package for optimizing selection problems, particularly in the context of training population design for genomic prediction and breeding programs.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Installation](#Installation)\n",
    "3. [Basic Concepts](#Basic-Concepts)\n",
    "4. [Unordered Subset Selection](#Unordered-Subset-Selection)\n",
    "5. [Ordered Subset Selection](#Ordered-Subset-Selection)\n",
    "6. [Combinatorial Optimization](#Combinatorial-Optimization)\n",
    "7. [Single-Objective Optimization with Multiple Criteria](#Single-Objective-Optimization-with-Multiple-Criteria)\n",
    "8. [Genomic Selection Applications](#Genomic-Selection-Applications)\n",
    "9. [Advanced Features](#Advanced-Features)\n",
    "10. [Custom Optimization Functions](#Custom-Optimization-Functions)\n",
    "11. [Performance Optimization](#Performance-Optimization)\n",
    "12. [Real-world Applications](#Real-world-Applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "TrainSelPy is a Python implementation of the TrainSel algorithm, designed for optimizing selection problems in various domains. It's particularly valuable for:\n",
    "\n",
    "- Selecting optimal training populations for genomic prediction\n",
    "- Solving combinatorial optimization problems\n",
    "- Optimizing selection problems with competing objectives\n",
    "- Handling ordered and unordered selection problems\n",
    "\n",
    "This vignette will guide you through the various features and applications of TrainSelPy, providing examples and explaining the underlying concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "TrainSelPy can be installed directly from the GitHub repository or using pip if it's available on PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TrainSelPy from GitHub (if not already installed)\n",
    "# !pip install git+https://github.com/dakdemirGithub/TrainSelPublic_StartingPoint.git#subdirectory=trainselpy\n",
    "\n",
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import TrainSelPy components\n",
    "from trainselpy import (\n",
    "    make_data,\n",
    "    train_sel,\n",
    "    train_sel_control,\n",
    "    set_control_default,\n",
    "    dopt,\n",
    "    cdmean_opt,\n",
    "    maximin_opt,\n",
    "    pev_opt,\n",
    "    time_estimation\n",
    ")\n",
    "\n",
    "# For visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "Before diving into examples, let's understand the core concepts of TrainSelPy:\n",
    "\n",
    "### Selection Types\n",
    "\n",
    "TrainSelPy supports multiple types of selection problems:\n",
    "\n",
    "1. **Unordered Set (UOS)**: Selection where order doesn't matter (e.g., selecting committee members)\n",
    "2. **Ordered Set (OS)**: Selection where order matters (e.g., sequential sampling)\n",
    "3. **Unordered Multiset (UOMS)**: Selection where order doesn't matter but repetition is allowed\n",
    "4. **Ordered Multiset (OMS)**: Selection where both order and repetition matter\n",
    "5. **Boolean (BOOL)**: Selection of binary variables (0/1)\n",
    "6. **Double (DBL)**: Selection of continuous variables (0-1)\n",
    "\n",
    "### Optimization Criteria\n",
    "\n",
    "TrainSelPy includes several built-in optimization criteria:\n",
    "\n",
    "1. **D-optimality**: Maximizing determinant of information matrix\n",
    "2. **CDMean**: Coefficient of Determination mean (for genomic prediction)\n",
    "3. **PEV**: Prediction Error Variance minimization\n",
    "4. **Maximin**: Maximizing minimum distance between selected samples\n",
    "\n",
    "### Optimization Algorithm\n",
    "\n",
    "TrainSelPy uses genetic algorithms for optimization, with features like:\n",
    "\n",
    "1. **Island Model**: Parallel populations evolving independently with migration\n",
    "2. **Simulated Annealing**: Fine-tuning solutions at the end of optimization\n",
    "3. **Custom Objective Functions**: Flexibility to define your own criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unordered Subset Selection\n",
    "\n",
    "The most common use case for TrainSelPy is unordered subset selection - selecting a subset of items from a larger set where the order doesn't matter.\n",
    "\n",
    "Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small test dataset\n",
    "n_samples = 100\n",
    "n_features = 20\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create a marker matrix (simulated genotypes)\n",
    "M = np.random.choice([-1, 0, 1], size=(n_samples, n_features), p=[0.25, 0.5, 0.25])\n",
    "\n",
    "# Create a relationship matrix (genomic relationships)\n",
    "K = np.dot(M, M.T) / n_features\n",
    "\n",
    "# Add a small value to the diagonal to ensure positive definiteness\n",
    "K += np.eye(n_samples) * 1e-6\n",
    "\n",
    "print(f\"Created dataset with {n_samples} samples and {n_features} features\")\n",
    "\n",
    "# Create the TrainSel data object\n",
    "ts_data = make_data(M=M)\n",
    "\n",
    "# For D-optimality, we need to add the feature matrix to the data\n",
    "ts_data[\"FeatureMat\"] = M\n",
    "\n",
    "# Set control parameters (limited iterations for example)\n",
    "control = set_control_default()\n",
    "control[\"niterations\"] = 30  # Reduced for demonstration\n",
    "control[\"npop\"] = 100\n",
    "\n",
    "# Run the selection algorithm with D-optimality\n",
    "result = train_sel(\n",
    "    data=ts_data,\n",
    "    candidates=[list(range(n_samples))],  # Select from all samples\n",
    "    setsizes=[10],                      # Select 10 samples\n",
    "    settypes=[\"UOS\"],                  # Unordered set\n",
    "    stat=dopt,                         # Use D-optimality\n",
    "    control=control,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSelected {len(result.selected_indices[0])} samples:\")\n",
    "print(f\"Selected indices: {result.selected_indices[0]}\")\n",
    "print(f\"Final fitness (D-optimality): {result.fitness:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Selection\n",
    "\n",
    "Let's visualize our selection using PCA to reduce dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use PCA to reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "M_reduced = pca.fit_transform(M)\n",
    "\n",
    "# Create a mask for selected samples\n",
    "selected_mask = np.zeros(n_samples, dtype=bool)\n",
    "selected_mask[result.selected_indices[0]] = True\n",
    "\n",
    "# Plot the samples in 2D space\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(M_reduced[~selected_mask, 0], M_reduced[~selected_mask, 1], alpha=0.5, label=\"Not Selected\")\n",
    "plt.scatter(M_reduced[selected_mask, 0], M_reduced[selected_mask, 1], color=\"red\", s=100, label=\"Selected\")\n",
    "plt.title(\"D-optimal Selection Visualization\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordered Subset Selection\n",
    "\n",
    "Let's now look at ordered subset selection (\"OS\"), which is useful for problems like the traveling salesman problem or sequential sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random distance matrix for a TSP-like problem\n",
    "n_cities = 20  # Reduced for faster execution\n",
    "np.random.seed(42)\n",
    "city_coords = np.random.rand(n_cities, 2) * 100  # Random coordinates in 2D space\n",
    "\n",
    "# Calculate distances between cities\n",
    "dist_matrix = np.zeros((n_cities, n_cities))\n",
    "for i in range(n_cities):\n",
    "    for j in range(n_cities):\n",
    "        dist_matrix[i, j] = np.sqrt(np.sum((city_coords[i] - city_coords[j])**2))\n",
    "\n",
    "# Convert to a form that TrainSelPy can use\n",
    "dist_df = pd.DataFrame(dist_matrix)\n",
    "\n",
    "# Create a data object for TrainSelPy\n",
    "ts_data = {\"DistMat\": dist_df}\n",
    "\n",
    "# Define a TSP fitness function (minimize total distance)\n",
    "def tsp_fitness(solution, data):\n",
    "    \"\"\"Calculate the negative total tour distance.\"\"\"\n",
    "    dist_mat = data[\"DistMat\"]\n",
    "    total_dist = 0\n",
    "    \n",
    "    # Calculate total tour distance including return to start\n",
    "    for i in range(len(solution)):\n",
    "        from_city = solution[i]\n",
    "        to_city = solution[(i + 1) % len(solution)]\n",
    "        total_dist += dist_mat.iloc[from_city, to_city]\n",
    "    \n",
    "    # Return negative distance (as TrainSelPy maximizes by default)\n",
    "    return -total_dist\n",
    "\n",
    "# Set control parameters\n",
    "control = set_control_default()\n",
    "control[\"niterations\"] = 50\n",
    "control[\"npop\"] = 200\n",
    "control[\"nelite\"] = 50\n",
    "control[\"mutprob\"] = 0.1\n",
    "control[\"crossprob\"] = 0.8\n",
    "\n",
    "# Run the TSP optimization\n",
    "start_time = time.time()\n",
    "result_tsp = train_sel(\n",
    "    data=ts_data,\n",
    "    candidates=[list(range(n_cities))],\n",
    "    setsizes=[n_cities],  # Select all cities (full tour)\n",
    "    settypes=[\"OS\"],      # Ordered set (the order matters)\n",
    "    stat=tsp_fitness,\n",
    "    control=control,\n",
    "    verbose=True\n",
    ")\n",
    "runtime = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTSP optimization completed in {runtime:.2f} seconds\")\n",
    "print(f\"Tour length: {-result_tsp.fitness:.2f} units\")\n",
    "print(f\"Tour: {result_tsp.selected_indices[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the TSP Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimized tour\n",
    "tour = result_tsp.selected_indices[0]\n",
    "\n",
    "# Plot the tour\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(city_coords[:, 0], city_coords[:, 1], s=100, c='blue', alpha=0.7)\n",
    "\n",
    "# Add city labels\n",
    "for i in range(n_cities):\n",
    "    plt.text(city_coords[i, 0] + 0.5, city_coords[i, 1] + 0.5, str(i), fontsize=12)\n",
    "\n",
    "# Plot the tour path\n",
    "for i in range(len(tour)):\n",
    "    from_city = tour[i]\n",
    "    to_city = tour[(i + 1) % len(tour)]\n",
    "    plt.plot([city_coords[from_city, 0], city_coords[to_city, 0]],\n",
    "             [city_coords[from_city, 1], city_coords[to_city, 1]], 'r-', alpha=0.6)\n",
    "\n",
    "plt.title(f\"Optimized TSP Tour (Length: {-result_tsp.fitness:.2f})\")\n",
    "plt.xlabel(\"X Coordinate\")\n",
    "plt.ylabel(\"Y Coordinate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinatorial Optimization\n",
    "\n",
    "TrainSelPy can handle various combinatorial optimization problems, including mixed integer problems where you have both discrete and continuous variables.\n",
    "\n",
    "Let's create a simple mixed integer problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mixed integer problem\n",
    "# - Select 5 items from a set of 20\n",
    "# - Assign continuous weights (0-1) to each selected item\n",
    "# - Maximize a weighted sum function\n",
    "\n",
    "n_items = 20\n",
    "n_select = 5\n",
    "n_weights = n_select\n",
    "\n",
    "# Create random item values\n",
    "np.random.seed(123)\n",
    "item_values = np.random.rand(n_items) * 10\n",
    "item_costs = np.random.rand(n_items) * 5\n",
    "\n",
    "# Create data object\n",
    "mixed_data = {\n",
    "    \"values\": item_values,\n",
    "    \"costs\": item_costs,\n",
    "    \"budget\": 10.0  # Maximum total cost\n",
    "}\n",
    "\n",
    "# Define mixed integer fitness function\n",
    "def mixed_integer_fitness(int_solution, dbl_solution, data):\n",
    "    \"\"\"Fitness function for mixed integer problem.\"\"\"\n",
    "    selected_items = int_solution\n",
    "    weights = dbl_solution\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    \n",
    "    # Calculate total value and cost\n",
    "    total_value = np.sum(data[\"values\"][selected_items] * weights)\n",
    "    total_cost = np.sum(data[\"costs\"][selected_items] * weights)\n",
    "    \n",
    "    # Apply penalty if over budget\n",
    "    if total_cost > data[\"budget\"]:\n",
    "        penalty = (total_cost - data[\"budget\"]) * 20\n",
    "        return total_value - penalty\n",
    "    \n",
    "    return total_value\n",
    "\n",
    "# Set control parameters\n",
    "control = set_control_default()\n",
    "control[\"niterations\"] = 30\n",
    "control[\"npop\"] = 100\n",
    "\n",
    "# Run the mixed integer optimization\n",
    "result_mixed = train_sel(\n",
    "    data=mixed_data,\n",
    "    candidates=[\n",
    "        list(range(n_items)),  # Integer candidates\n",
    "        list(range(n_weights))  # Just placeholder for continuous vars\n",
    "    ],\n",
    "    setsizes=[n_select, n_weights],\n",
    "    settypes=[\"UOS\", \"DBL\"],  # UOS for items, DBL for weights\n",
    "    stat=mixed_integer_fitness,\n",
    "    control=control,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Get results\n",
    "selected_items = result_mixed.selected_indices[0]\n",
    "selected_weights_raw = result_mixed.selected_values[0]\n",
    "selected_weights = np.array(selected_weights_raw) / np.sum(selected_weights_raw)\n",
    "\n",
    "# Calculate final metrics\n",
    "total_value = np.sum(item_values[selected_items] * selected_weights)\n",
    "total_cost = np.sum(item_costs[selected_items] * selected_weights)\n",
    "\n",
    "print(f\"\\nSelected items: {selected_items}\")\n",
    "print(f\"Normalized weights: {selected_weights}\")\n",
    "print(f\"Total value: {total_value:.2f}\")\n",
    "print(f\"Total cost: {total_cost:.2f} (Budget: {mixed_data['budget']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Mixed Integer Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all items\n",
    "all_items_df = pd.DataFrame({\n",
    "    'Item': range(n_items),\n",
    "    'Value': item_values,\n",
    "    'Cost': item_costs,\n",
    "    'Value/Cost': item_values / item_costs,\n",
    "    'Selected': ['Yes' if i in selected_items else 'No' for i in range(n_items)]\n",
    "})\n",
    "\n",
    "# Create a dataframe with just the selected items and their weights\n",
    "selected_df = pd.DataFrame({\n",
    "    'Item': selected_items,\n",
    "    'Value': item_values[selected_items],\n",
    "    'Cost': item_costs[selected_items],\n",
    "    'Weight': selected_weights,\n",
    "    'Weighted Value': item_values[selected_items] * selected_weights,\n",
    "    'Weighted Cost': item_costs[selected_items] * selected_weights\n",
    "})\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"All Items:\")\n",
    "display(all_items_df)\n",
    "\n",
    "print(\"\\nSelected Items:\")\n",
    "display(selected_df)\n",
    "\n",
    "# Visualize the value-cost tradeoff\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(item_costs, item_values, s=80, alpha=0.6, label=\"Not Selected\")\n",
    "plt.scatter(item_costs[selected_items], item_values[selected_items], s=150, c='red', alpha=0.7, label=\"Selected\")\n",
    "\n",
    "# Add item labels\n",
    "for i in range(n_items):\n",
    "    plt.text(item_costs[i] + 0.1, item_values[i] + 0.1, str(i), fontsize=10)\n",
    "\n",
    "# Add weighted positions\n",
    "weighted_cost = np.sum(item_costs[selected_items] * selected_weights)\n",
    "weighted_value = np.sum(item_values[selected_items] * selected_weights)\n",
    "plt.scatter([weighted_cost], [weighted_value], s=200, c='green', marker='*', label=\"Weighted Result\")\n",
    "\n",
    "plt.axvline(x=mixed_data['budget'], color='black', linestyle='--', label=\"Budget Limit\")\n",
    "\n",
    "plt.title(\"Item Selection with Weights Optimization\")\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Objective Optimization with Multiple Criteria\n",
    "\n",
    "TrainSelPy can combine multiple criteria into a single objective function. This approach is useful when you want to balance multiple criteria without going into full multi-objective optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated breeding population dataset\n",
    "n_individuals = 100\n",
    "n_markers = 50\n",
    "\n",
    "np.random.seed(123)\n",
    "# Simulate marker data\n",
    "M = np.random.choice([-1, 0, 1], size=(n_individuals, n_markers), p=[0.25, 0.5, 0.25])\n",
    "\n",
    "# Create a relationship matrix (genomic relationships)\n",
    "K = np.dot(M, M.T) / n_markers\n",
    "\n",
    "# Add small diagonal value for numerical stability\n",
    "K += np.eye(n_individuals) * 1e-6\n",
    "\n",
    "# Simulate breeding values\n",
    "breeding_values = np.random.normal(0, 1, size=n_individuals)\n",
    "breeding_values = breeding_values * 10 + 100  # Scale for better visualization\n",
    "\n",
    "# Create data object\n",
    "breeding_data = {\n",
    "    \"breeding_values\": breeding_values,\n",
    "    \"K\": K\n",
    "}\n",
    "\n",
    "# Define a function that balances genetic gain and diversity\n",
    "def balanced_selection(solution, data):\n",
    "    \"\"\"Objective function balancing genetic gain and diversity.\"\"\"\n",
    "    breeding_values = data[\"breeding_values\"]\n",
    "    K = data[\"K\"]\n",
    "    \n",
    "    # Calculate genetic gain (mean breeding value of selected individuals)\n",
    "    genetic_gain = np.mean(breeding_values[solution])\n",
    "    \n",
    "    # Calculate inbreeding (mean relationship coefficient among selected)\n",
    "    selected_K = K[np.ix_(solution, solution)]\n",
    "    # Get off-diagonal elements (relationships between different individuals)\n",
    "    mask = ~np.eye(selected_K.shape[0], dtype=bool)\n",
    "    inbreeding = np.mean(selected_K[mask])\n",
    "    \n",
    "    # Balance the two objectives (maximize gain, minimize inbreeding)\n",
    "    # Scale genetic gain to be roughly in the same range as inbreeding\n",
    "    scaled_gain = (genetic_gain - 90) / 20  # Assuming breeding values around 100\n",
    "    \n",
    "    # Weighted combination\n",
    "    return 0.7 * scaled_gain - 0.3 * inbreeding\n",
    "\n",
    "# Set control parameters\n",
    "control = set_control_default()\n",
    "control[\"niterations\"] = 30\n",
    "control[\"npop\"] = 150\n",
    "\n",
    "# Run the optimization\n",
    "result_balanced = train_sel(\n",
    "    data=breeding_data,\n",
    "    candidates=[list(range(n_individuals))],\n",
    "    setsizes=[10],  # Select 10 individuals\n",
    "    settypes=[\"UOS\"],\n",
    "    stat=balanced_selection,\n",
    "    control=control,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print the selected individuals\n",
    "selected = result_balanced.selected_indices[0]\n",
    "print(f\"\\nSelected individuals: {selected}\")\n",
    "print(f\"Fitness: {result_balanced.fitness:.4f}\")\n",
    "\n",
    "# Calculate the individual components\n",
    "genetic_gain = np.mean(breeding_values[selected])\n",
    "selected_K = K[np.ix_(selected, selected)]\n",
    "mask = ~np.eye(selected_K.shape[0], dtype=bool)\n",
    "inbreeding = np.mean(selected_K[mask])\n",
    "\n",
    "print(f\"Genetic gain: {genetic_gain:.4f}\")\n",
    "print(f\"Average inbreeding: {inbreeding:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Random Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with random selections\n",
    "n_random = 100\n",
    "random_gains = []\n",
    "random_inbreedings = []\n",
    "\n",
    "for _ in range(n_random):\n",
    "    random_selection = np.random.choice(n_individuals, size=10, replace=False)\n",
    "    \n",
    "    # Calculate gain\n",
    "    gain = np.mean(breeding_values[random_selection])\n",
    "    random_gains.append(gain)\n",
    "    \n",
    "    # Calculate inbreeding\n",
    "    sel_K = K[np.ix_(random_selection, random_selection)]\n",
    "    mask = ~np.eye(sel_K.shape[0], dtype=bool)\n",
    "    inb = np.mean(sel_K[mask])\n",
    "    random_inbreedings.append(inb)\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(random_inbreedings, random_gains, alpha=0.5, label=\"Random Selections\")\n",
    "plt.scatter([inbreeding], [genetic_gain], color='red', s=100, label=\"Optimized Selection\")\n",
    "\n",
    "plt.title(\"Genetic Gain vs. Inbreeding\")\n",
    "plt.xlabel(\"Inbreeding (Average Relationship)\")\n",
    "plt.ylabel(\"Genetic Gain (Average Breeding Value)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breeding Example with Mixed Variable Types\n",
    "\n",
    "In breeding applications, we often need to select individuals and assign mating proportions. This is a mixed type problem with both integer variables (which individuals to select) and continuous variables (mating proportions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-objective function for optimal contribution selection\n",
    "def optimal_contributions(int_sol, dbl_sol, data):\n",
    "    \"\"\"Calculate a combined objective for gain and inbreeding with contribution proportions.\"\"\"\n",
    "    # Normalize the contributions (proportions)\n",
    "    props = np.array(dbl_sol) / np.sum(dbl_sol)\n",
    "    \n",
    "    breeding_values = data[\"breeding_values\"]\n",
    "    K = data[\"K\"]\n",
    "    \n",
    "    # Weighted genetic gain\n",
    "    genetic_gain = np.sum(breeding_values[int_sol] * props)\n",
    "    \n",
    "    # Expected inbreeding (quadratic form: p'Kp)\n",
    "    selected_K = K[np.ix_(int_sol, int_sol)]\n",
    "    inbreeding = props.T @ selected_K @ props\n",
    "    \n",
    "    # Combine objectives (with appropriate scaling)\n",
    "    scaled_gain = (genetic_gain - 90) / 20\n",
    "    return 0.7 * scaled_gain - 0.3 * inbreeding\n",
    "\n",
    "# Optimize both selection and contributions\n",
    "result_ocs = train_sel(\n",
    "    data=breeding_data,\n",
    "    candidates=[\n",
    "        list(range(n_individuals)),  # Integer candidates\n",
    "        list(range(5))               # Just placeholder for 5 continuous variables\n",
    "    ],\n",
    "    setsizes=[5, 5],                 # Select 5 individuals with 5 contribution values\n",
    "    settypes=[\"UOS\", \"DBL\"],         # UOS for individuals, DBL for contributions\n",
    "    stat=optimal_contributions,\n",
    "    control=control,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "selected_inds = result_ocs.selected_indices[0]\n",
    "contribution_raw = result_ocs.selected_values[0]\n",
    "contributions = np.array(contribution_raw) / np.sum(contribution_raw)\n",
    "\n",
    "print(f\"\\nSelected individuals: {selected_inds}\")\n",
    "print(f\"Contributions: {contributions}\")\n",
    "\n",
    "# Calculate final metrics\n",
    "genetic_gain = np.sum(breeding_values[selected_inds] * contributions)\n",
    "selected_K = K[np.ix_(selected_inds, selected_inds)]\n",
    "inbreeding = contributions.T @ selected_K @ contributions\n",
    "\n",
    "print(f\"Genetic gain: {genetic_gain:.4f}\")\n",
    "print(f\"Expected inbreeding: {inbreeding:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genomic Selection Applications\n",
    "\n",
    "Let's explore how TrainSelPy can be used for genomic selection, particularly for training population optimization. We'll simulate a breeding population and use TrainSelPy to optimize the training set for genomic prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Function to simulate breeding data with genetic values and phenotypes\n",
    "def simulate_breeding_data(n_individuals=200, n_markers=300, h2=0.5):\n",
    "    \"\"\"Simulate a breeding population with markers and phenotypes.\"\"\"\n",
    "    # Simulate marker matrix\n",
    "    M = np.random.choice([-1, 0, 1], size=(n_individuals, n_markers), p=[0.25, 0.5, 0.25])\n",
    "    \n",
    "    # Simulate QTL effects\n",
    "    qtl_effects = np.zeros(n_markers)\n",
    "    # 10% of markers are QTLs\n",
    "    qtl_indices = np.random.choice(n_markers, size=int(n_markers * 0.1), replace=False)\n",
    "    qtl_effects[qtl_indices] = np.random.normal(0, 1, size=len(qtl_indices))\n",
    "    \n",
    "    # Calculate genetic values\n",
    "    genetic_values = np.dot(M, qtl_effects)\n",
    "    \n",
    "    # Add environmental noise based on heritability\n",
    "    genetic_var = np.var(genetic_values)\n",
    "    error_var = genetic_var * (1 - h2) / h2\n",
    "    env_effects = np.random.normal(0, np.sqrt(error_var), size=n_individuals)\n",
    "    \n",
    "    # Calculate phenotypes\n",
    "    phenotypes = genetic_values + env_effects\n",
    "    \n",
    "    # Calculate genomic relationship matrix\n",
    "    K = np.dot(M, M.T) / n_markers\n",
    "    K += np.eye(n_individuals) * 1e-6  # Add small diagonal for numerical stability\n",
    "    \n",
    "    return {\n",
    "        'M': M,\n",
    "        'K': K,\n",
    "        'genetic_values': genetic_values,\n",
    "        'phenotypes': phenotypes,\n",
    "        'qtl_effects': qtl_effects\n",
    "    }\n",
    "\n",
    "# Simulate breeding data\n",
    "np.random.seed(123)\n",
    "sim_data = simulate_breeding_data(n_individuals=200, n_markers=300, h2=0.5)\n",
    "\n",
    "# Split into training candidates and test set\n",
    "all_indices = np.arange(200)\n",
    "candidate_indices, test_indices = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training candidates: {len(candidate_indices)}\")\n",
    "print(f\"Test set: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimization Function for Genomic Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fitness function for prediction accuracy\n",
    "def prediction_accuracy(training_indices, data):\n",
    "    \"\"\"Evaluate prediction accuracy with a given training set.\"\"\"\n",
    "    # Extract data\n",
    "    M = data[\"M\"]\n",
    "    phenotypes = data[\"phenotypes\"]\n",
    "    true_genetic_values = data[\"genetic_values\"]\n",
    "    test_indices = data[\"test_indices\"]\n",
    "    \n",
    "    # Prepare training and test sets\n",
    "    X_train = M[training_indices, :]\n",
    "    y_train = phenotypes[training_indices]\n",
    "    X_test = M[test_indices, :]\n",
    "    y_test_true = true_genetic_values[test_indices]\n",
    "    \n",
    "    # Train a Ridge regression model (commonly used in genomic prediction)\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict genetic values for the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate prediction accuracy as correlation\n",
    "    accuracy = np.corrcoef(y_test_true, y_test_pred)[0, 1]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Prepare data for TrainSelPy\n",
    "gs_data = {\n",
    "    \"M\": sim_data[\"M\"],\n",
    "    \"phenotypes\": sim_data[\"phenotypes\"],\n",
    "    \"genetic_values\": sim_data[\"genetic_values\"],\n",
    "    \"test_indices\": test_indices\n",
    "}\n",
    "\n",
    "# Set control parameters\n",
    "control = set_control_default()\n",
    "control[\"niterations\"] = 30\n",
    "control[\"npop\"] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Training Population Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different training set sizes, but fewer for speed\n",
    "training_sizes = [20, 40, 60]\n",
    "random_accuracies = []\n",
    "optimized_accuracies = []\n",
    "\n",
    "for size in training_sizes:\n",
    "    print(f\"\\nTraining set size: {size}\")\n",
    "    \n",
    "    # Random selection (average of 3 repeats)\n",
    "    print(\"  Random selection:\")\n",
    "    random_accuracy_sum = 0\n",
    "    n_repeats = 3  # Reduced for speed\n",
    "    \n",
    "    for i in range(n_repeats):\n",
    "        random_indices = np.random.choice(candidate_indices, size=size, replace=False)\n",
    "        accuracy = prediction_accuracy(random_indices, gs_data)\n",
    "        random_accuracy_sum += accuracy\n",
    "        print(f\"    Repeat {i+1}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    avg_random_accuracy = random_accuracy_sum / n_repeats\n",
    "    random_accuracies.append(avg_random_accuracy)\n",
    "    print(f\"  Average random selection accuracy: {avg_random_accuracy:.4f}\")\n",
    "    \n",
    "    # Optimized selection\n",
    "    print(\"  Optimized selection:\")\n",
    "    result = train_sel(\n",
    "        data=gs_data,\n",
    "        candidates=[candidate_indices.tolist()],\n",
    "        setsizes=[size],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=prediction_accuracy,\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    optimized_accuracy = result.fitness\n",
    "    optimized_accuracies.append(optimized_accuracy)\n",
    "    print(f\"  Optimized selection accuracy: {optimized_accuracy:.4f}\")\n",
    "    print(f\"  Improvement: {(optimized_accuracy - avg_random_accuracy) / avg_random_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Genomic Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(training_sizes, random_accuracies, 'o-', label='Random Selection', linewidth=2)\n",
    "plt.plot(training_sizes, optimized_accuracies, 'o-', label='Optimized Selection', linewidth=2)\n",
    "plt.title('Prediction Accuracy vs. Training Set Size', fontsize=16)\n",
    "plt.xlabel('Training Set Size', fontsize=14)\n",
    "plt.ylabel('Prediction Accuracy', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('genomic_selection_results.png')\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentage improvements\n",
    "improvements = [(opt - rand) / rand * 100 for opt, rand in zip(optimized_accuracies, random_accuracies)]\n",
    "\n",
    "# Display as a table\n",
    "results_df = pd.DataFrame({\n",
    "    'Training Size': training_sizes,\n",
    "    'Random Selection': random_accuracies,\n",
    "    'Optimized Selection': optimized_accuracies,\n",
    "    'Improvement (%)': improvements\n",
    "})\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "TrainSelPy includes several advanced features that can enhance optimization performance:\n",
    "\n",
    "1. Island Model Parallelization\n",
    "2. Dynamic Adjustment of Elite Population\n",
    "3. Simulated Annealing Refinement\n",
    "\n",
    "Let's demonstrate the island model parallelization for a more complex problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger problem for demonstrating parallel processing\n",
    "n_samples = 200  # Reduced for speed\n",
    "n_features = 50  # Reduced for speed\n",
    "\n",
    "# Create a marker matrix\n",
    "np.random.seed(42)\n",
    "M_large = np.random.choice([-1, 0, 1], size=(n_samples, n_features), p=[0.25, 0.5, 0.25])\n",
    "\n",
    "# Create the TrainSel data object\n",
    "ts_data_large = make_data(M=M_large)\n",
    "ts_data_large[\"FeatureMat\"] = M_large\n",
    "\n",
    "# Set control parameters for island model\n",
    "parallel_control = train_sel_control(\n",
    "    size=\"demo\",\n",
    "    niterations=20,  # Reduced for demonstration\n",
    "    npop=100,\n",
    "    nislands=2,      # Use 2 islands\n",
    "    parallelizable=True,\n",
    "    mc_cores=2       # Use 2 cores\n",
    ")\n",
    "\n",
    "# Set control parameters for single-population version\n",
    "single_control = train_sel_control(\n",
    "    size=\"demo\",\n",
    "    niterations=40,  # 2x iterations to give fair comparison\n",
    "    npop=100\n",
    ")\n",
    "\n",
    "# Run optimization with island model\n",
    "print(\"Running island model optimization...\")\n",
    "start_time = time.time()\n",
    "result_island = train_sel(\n",
    "    data=ts_data_large,\n",
    "    candidates=[list(range(n_samples))],\n",
    "    setsizes=[20],\n",
    "    settypes=[\"UOS\"],\n",
    "    stat=dopt,\n",
    "    control=parallel_control,\n",
    "    n_jobs=2,  # Use 2 parallel jobs\n",
    "    verbose=True\n",
    ")\n",
    "island_time = time.time() - start_time\n",
    "\n",
    "# Run optimization with single population\n",
    "print(\"\\nRunning single-population optimization...\")\n",
    "start_time = time.time()\n",
    "result_single = train_sel(\n",
    "    data=ts_data_large,\n",
    "    candidates=[list(range(n_samples))],\n",
    "    setsizes=[20],\n",
    "    settypes=[\"UOS\"],\n",
    "    stat=dopt,\n",
    "    control=single_control,\n",
    "    verbose=True\n",
    ")\n",
    "single_time = time.time() - start_time\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nIsland model runtime: {island_time:.2f} seconds\")\n",
    "print(f\"Single population runtime: {single_time:.2f} seconds\")\n",
    "print(f\"Speedup: {single_time / island_time:.2f}x\")\n",
    "\n",
    "print(f\"\\nIsland model fitness: {result_island.fitness:.6f}\")\n",
    "print(f\"Single population fitness: {result_single.fitness:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence history\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Normalize iterations to common scale (0-1)\n",
    "x_island = np.linspace(0, 1, len(result_island.fitness_history))\n",
    "x_single = np.linspace(0, 1, len(result_single.fitness_history))\n",
    "\n",
    "plt.plot(x_island, result_island.fitness_history, 'b-', label='Island Model', linewidth=2)\n",
    "plt.plot(x_single, result_single.fitness_history, 'r-', label='Single Population', linewidth=2)\n",
    "\n",
    "plt.title('Convergence Comparison: Island Model vs. Single Population', fontsize=16)\n",
    "plt.xlabel('Normalized Iterations', fontsize=14)\n",
    "plt.ylabel('Fitness (D-optimality)', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Optimization Functions\n",
    "\n",
    "TrainSelPy is highly extensible, allowing you to define custom optimization criteria. Let's demonstrate how to create a custom optimization function that combines multiple criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for custom optimization\n",
    "n_samples = 100\n",
    "n_features = 30\n",
    "\n",
    "np.random.seed(42)\n",
    "# Create a marker matrix\n",
    "M_custom = np.random.choice([-1, 0, 1], size=(n_samples, n_features), p=[0.25, 0.5, 0.25])\n",
    "\n",
    "# Calculate distance matrix\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "dist_matrix = squareform(pdist(M_custom))\n",
    "\n",
    "# Create a TrainSel data object\n",
    "ts_data_custom = make_data(M=M_custom)\n",
    "ts_data_custom[\"FeatureMat\"] = M_custom\n",
    "ts_data_custom[\"DistMat\"] = pd.DataFrame(dist_matrix)\n",
    "\n",
    "# Define a custom optimization function combining D-optimality and maximin criteria\n",
    "def custom_composite_criterion(solution, data):\n",
    "    \"\"\"Custom criterion combining D-optimality and diversity.\"\"\"\n",
    "    # Calculate D-optimality\n",
    "    dopt_value = dopt(solution, data)\n",
    "    \n",
    "    # Calculate maximin criterion (diversity)\n",
    "    maximin_value = maximin_opt(solution, data)\n",
    "    \n",
    "    # Scale the objectives to make them comparable\n",
    "    # We're aiming to maximize both\n",
    "    scaled_dopt = dopt_value / 100  # Scale down D-optimality to match maximin range\n",
    "    \n",
    "    # Return weighted combination (can adjust weights to emphasize one objective more)\n",
    "    return 0.7 * scaled_dopt + 0.3 * maximin_value\n",
    "\n",
    "# Run the optimization with the custom criterion\n",
    "result_custom = train_sel(\n",
    "    data=ts_data_custom,\n",
    "    candidates=[list(range(n_samples))],\n",
    "    setsizes=[15],\n",
    "    settypes=[\"UOS\"],\n",
    "    stat=custom_composite_criterion,\n",
    "    control=set_control_default(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Calculate individual criteria values for the solution\n",
    "selected = result_custom.selected_indices[0]\n",
    "dopt_val = dopt(selected, ts_data_custom)\n",
    "maximin_val = maximin_opt(selected, ts_data_custom)\n",
    "\n",
    "print(f\"\\nSelected {len(selected)} samples: {selected}\")\n",
    "print(f\"Composite fitness: {result_custom.fitness:.6f}\")\n",
    "print(f\"D-optimality: {dopt_val:.6f}\")\n",
    "print(f\"Maximin diversity: {maximin_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different Optimization Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different criteria on the same dataset\n",
    "criteria = [\n",
    "    (\"D-optimality\", dopt),\n",
    "    (\"Maximin\", maximin_opt),\n",
    "    (\"Composite\", custom_composite_criterion)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, criterion in criteria:\n",
    "    print(f\"\\nRunning optimization with {name} criterion...\")\n",
    "    \n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 30  # Reduced for demonstration\n",
    "    \n",
    "    result = train_sel(\n",
    "        data=ts_data_custom,\n",
    "        candidates=[list(range(n_samples))],\n",
    "        setsizes=[15],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=criterion,\n",
    "        control=control,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    selected = result.selected_indices[0]\n",
    "    results[name] = {\n",
    "        \"selected\": selected,\n",
    "        \"fitness\": result.fitness,\n",
    "        \"dopt\": dopt(selected, ts_data_custom),\n",
    "        \"maximin\": maximin_opt(selected, ts_data_custom)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Fitness: {result.fitness:.6f}\")\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Criterion': [name for name, _ in criteria],\n",
    "    'D-optimality': [results[name]['dopt'] for name, _ in criteria],\n",
    "    'Maximin': [results[name]['maximin'] for name, _ in criteria]\n",
    "})\n",
    "\n",
    "print(\"\\nComparison of different criteria:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize the results using PCA\n",
    "pca = PCA(n_components=2)\n",
    "M_reduced = pca.fit_transform(M_custom)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (name, _) in enumerate(criteria):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    selected = results[name]['selected']\n",
    "    selected_mask = np.zeros(n_samples, dtype=bool)\n",
    "    selected_mask[selected] = True\n",
    "    \n",
    "    plt.scatter(M_reduced[~selected_mask, 0], M_reduced[~selected_mask, 1], alpha=0.5, label=\"Not Selected\")\n",
    "    plt.scatter(M_reduced[selected_mask, 0], M_reduced[selected_mask, 1], color=\"red\", s=100, label=\"Selected\")\n",
    "    \n",
    "    plt.title(f\"{name} Selection\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "TrainSelPy offers several ways to improve performance for large-scale problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate performance optimization strategies\n",
    "print(\"TrainSelPy Performance Optimization Strategies:\")\n",
    "print(\"\\n1. Island Model Parallelization\")\n",
    "print(\"   - Set nislands > 1 and parallelizable=True\")\n",
    "print(\"   - Example: control = train_sel_control(nislands=4, parallelizable=True, mc_cores=4)\")\n",
    "print(\"   - Use n_jobs parameter in train_sel() to specify the number of parallel jobs\")\n",
    "\n",
    "print(\"\\n2. Adjust Genetic Algorithm Parameters\")\n",
    "print(\"   - Increase population size (npop) for better exploration\")\n",
    "print(\"   - Adjust crossover and mutation rates (crossprob, mutprob)\")\n",
    "print(\"   - Example: control = train_sel_control(npop=500, crossprob=0.8, mutprob=0.1)\")\n",
    "\n",
    "print(\"\\n3. Use Dynamic Elite Size Adjustment\")\n",
    "print(\"   - Enables automatic adjustment of elite population\")\n",
    "print(\"   - Example: control = train_sel_control(dynamicNelite=True)\")\n",
    "\n",
    "print(\"\\n4. Simulated Annealing Refinement\")\n",
    "print(\"   - Fine-tunes solutions after genetic algorithm\")\n",
    "print(\"   - Adjust parameters: niterSANN, tempini, tempfin\")\n",
    "print(\"   - Example: control = train_sel_control(niterSANN=100, tempini=100.0, tempfin=0.1)\")\n",
    "\n",
    "print(\"\\n5. Time Estimation\")\n",
    "# Demonstrate time estimation\n",
    "\n",
    "# Estimate time for different problem sizes\n",
    "problem_sizes = [(100, 10), (500, 50), (1000, 100)]\n",
    "\n",
    "print(\"\\nEstimated optimization times:\")\n",
    "for n_ind, n_sel in problem_sizes:\n",
    "    time_est = time_estimation(n_ind, n_sel, niter=100)\n",
    "    print(f\"  {n_ind} individuals, select {n_sel}: {time_est:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Applications\n",
    "\n",
    "Let's summarize some real-world applications of TrainSelPy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Genomic Selection\n",
    "\n",
    "TrainSelPy is highly effective for optimizing training populations in genomic selection:\n",
    "\n",
    "- Selecting individuals for phenotyping\n",
    "- Balancing genetic gain and diversity\n",
    "- Optimizing multi-environment trials\n",
    "- Allocating resources across multiple traits\n",
    "\n",
    "### 2. Experimental Design\n",
    "\n",
    "TrainSelPy can optimize experimental designs:\n",
    "\n",
    "- D-optimal designs\n",
    "- Sequential experimentation\n",
    "- Multi-stage designs\n",
    "- Balanced incomplete block designs\n",
    "\n",
    "### 3. Operations Research\n",
    "\n",
    "TrainSelPy can solve various combinatorial optimization problems:\n",
    "\n",
    "- Traveling salesman problem\n",
    "- Knapsack problems\n",
    "- Resource allocation\n",
    "- Portfolio optimization\n",
    "\n",
    "### 4. Machine Learning\n",
    "\n",
    "TrainSelPy supports machine learning applications:\n",
    "\n",
    "- Feature selection\n",
    "- Active learning\n",
    "- Training data subset selection\n",
    "- Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This vignette has demonstrated the core features and applications of TrainSelPy:\n",
    "\n",
    "1. **Selection Types**: UOS, OS, UOMS, OMS, BOOL, and DBL\n",
    "2. **Optimization Criteria**: D-optimality, Maximin, PEV, CDMean, and custom functions\n",
    "3. **Combinatorial Optimization**: Mixed integer problems with both discrete and continuous variables\n",
    "4. **Balancing Multiple Objectives**: Handling competing objectives like gain and diversity\n",
    "5. **Genomic Selection**: Training population optimization for breeding\n",
    "6. **Performance Optimization**: Island model and parallel processing\n",
    "\n",
    "TrainSelPy offers a flexible and powerful framework for solving a wide range of selection and optimization problems, with particular strengths in applications to breeding and genomic selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}