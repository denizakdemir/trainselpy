{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal A/B Test Design Using Mixed Models and TrainSelPy\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook bridges classical experimental design optimization with modern A/B testing by formulating digital experiments as mixed-effects models and applying TrainSelPy's CDMean and PEV criteria for optimal participant selection. We demonstrate how genetic algorithm optimization can dramatically improve experimental efficiency and external validity compared to traditional random sampling approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### 1.1 Limitations of Traditional A/B Testing Design\n",
    "\n",
    "Traditional A/B testing relies on **simple random assignment** with the implicit assumption that:\n",
    "1. All users are equally informative\n",
    "2. Random sampling guarantees representativeness\n",
    "3. Larger samples always yield better estimates\n",
    "\n",
    "However, in practice:\n",
    "- **Testing capacity is limited** (infrastructure, ethics, cost)\n",
    "- **Users are heterogeneous** with varying response patterns  \n",
    "- **Network effects** create dependencies between users\n",
    "- **External validity** to broader populations is uncertain\n",
    "\n",
    "### 1.2 The Optimization Opportunity\n",
    "\n",
    "Instead of random sampling, we can **optimize participant selection** to:\n",
    "- Maximize statistical power with fewer participants\n",
    "- Improve generalizability to target populations\n",
    "- Balance multiple experimental objectives\n",
    "- Account for user heterogeneity and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixed Models in A/B Testing\n",
    "\n",
    "### 2.1 Mathematical Formulation\n",
    "\n",
    "Digital experiments naturally exhibit **hierarchical structure**:\n",
    "\n",
    "$$Y_{ijk} = \\mu + \\tau_j + \\beta'X_{ijk} + u_i + \\epsilon_{ijk}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{ijk}$ = outcome for user $i$, treatment $j$, observation $k$\n",
    "- $\\mu$ = overall intercept\n",
    "- $\\tau_j$ = fixed treatment effect for treatment $j$\n",
    "- $\\beta'X_{ijk}$ = fixed effects of covariates\n",
    "- $u_i \\sim N(0, \\sigma_u^2)$ = random user effect\n",
    "- $\\epsilon_{ijk} \\sim N(0, \\sigma_e^2)$ = residual error\n",
    "\n",
    "### 2.2 Matrix Formulation\n",
    "\n",
    "In matrix form:\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ = $(n \\times 1)$ vector of responses\n",
    "- $\\mathbf{X}$ = $(n \\times p)$ design matrix for fixed effects\n",
    "- $\\mathbf{Z}$ = $(n \\times q)$ design matrix for random effects\n",
    "- $\\boldsymbol{\\beta}$ = $(p \\times 1)$ vector of fixed effects\n",
    "- $\\mathbf{u} \\sim N(\\mathbf{0}, \\mathbf{G})$ = $(q \\times 1)$ vector of random effects\n",
    "- $\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\mathbf{R})$ = $(n \\times 1)$ vector of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mixed Model Equations\n",
    "\n",
    "The **Best Linear Unbiased Predictors (BLUPs)** are obtained by solving:\n",
    "\n",
    "$$\\begin{bmatrix} \n",
    "\\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{Z} \\\\\n",
    "\\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{Z} + \\mathbf{G}^{-1}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\hat{\\boldsymbol{\\beta}} \\\\ \n",
    "\\hat{\\mathbf{u}} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{y} \\\\ \n",
    "\\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{y} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### 2.4 Why Mixed Models Matter in A/B Testing\n",
    "\n",
    "1. **User Heterogeneity**: Random user effects $u_i$ capture individual differences\n",
    "2. **Repeated Measurements**: Multiple sessions/actions per user create clustering\n",
    "3. **Generalization**: Mixed models explicitly model population-level effects\n",
    "4. **Efficiency**: Proper modeling of correlation structure improves precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coefficient of Determination Mean (CDMean)\n",
    "\n",
    "### 3.1 Mathematical Definition\n",
    "\n",
    "For a mixed model with training set $T$ and prediction set $P$, the **Coefficient of Determination (CD)** for individual $i$ is:\n",
    "\n",
    "$$CD_i = 1 - \\frac{PEV_i}{\\sigma_u^2}$$\n",
    "\n",
    "Where $PEV_i$ is the **Prediction Error Variance**:\n",
    "$$PEV_i = \\text{Var}(\\hat{u}_i - u_i | \\text{training data})$$\n",
    "\n",
    "The **CDMean** criterion is:\n",
    "$$\\text{CDMean} = \\frac{1}{|P|} \\sum_{i \\in P} CD_i = 1 - \\frac{1}{|P|} \\sum_{i \\in P} \\frac{PEV_i}{\\sigma_u^2}$$\n",
    "\n",
    "### 3.2 TrainSelPy Implementation\n",
    "\n",
    "The CDMean in TrainSelPy is computed as:\n",
    "\n",
    "$$\\text{CDMean} = \\frac{1}{n-|T|} \\sum_{i \\notin T} \\mathbf{g}_i' (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\left[\\mathbf{I} - \\frac{\\mathbf{1}\\mathbf{1}'}{1'\\mathbf{1}}\\right] (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\mathbf{g}_i / g_{ii}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{G}_{TT}$ = relationship matrix among training individuals\n",
    "- $\\mathbf{g}_i$ = relationship vector between individual $i$ and training set\n",
    "- $\\lambda = \\sigma_e^2/\\sigma_u^2$ = variance ratio\n",
    "- $g_{ii}$ = diagonal element of $\\mathbf{G}$ for individual $i$\n",
    "\n",
    "### 3.3 Interpretation in A/B Testing\n",
    "\n",
    "CDMean answers: **\"How well can our experimental participants predict treatment effects for non-participants?\"**\n",
    "\n",
    "- **CDMean = 1**: Perfect prediction (experimental group perfectly represents population)\n",
    "- **CDMean = 0**: No predictive value (experimental group uninformative)\n",
    "- **Higher CDMean**: Better external validity and generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Error Variance (PEV)\n",
    "\n",
    "### 4.1 Mathematical Definition\n",
    "\n",
    "For individual $i$ not in the training set, the **PEV** is:\n",
    "\n",
    "$$PEV_i = \\text{Var}(\\hat{u}_i | \\text{training data}) = g_{ii} - \\mathbf{g}_i' (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\mathbf{g}_i$$\n",
    "\n",
    "The **mean PEV** criterion minimizes:\n",
    "$$\\text{Mean PEV} = \\frac{1}{|P|} \\sum_{i \\in P} PEV_i$$\n",
    "\n",
    "### 4.2 Interpretation in A/B Testing\n",
    "\n",
    "PEV answers: **\"How uncertain are our predictions for non-participants?\"**\n",
    "\n",
    "- **Lower PEV**: More precise predictions for population\n",
    "- **Minimizing PEV**: Optimal selection for reducing prediction uncertainty\n",
    "- **Direct connection**: To confidence intervals for treatment effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Implementation with TrainSelPy\n",
    "\n",
    "### 5.1 Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from trainselpy import train_sel, make_data, cdmean_opt, pev_opt, dopt, set_control_default\n",
    "\n",
    "\n",
    "def generate_user_data(n_users: int = 1000, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic e-commerce user data with realistic marginal\n",
    "    distributions and one-hot region encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_users : int\n",
    "        Number of simulated users.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame comprising demographic, behavioural, device,\n",
    "        subscription and regional dummy variables.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # ── Demographics ───────────────────────────────────────────────────────────\n",
    "    age = np.clip(rng.gamma(shape=2.0, scale=15.0, size=n_users) + 18.0, 18, 80)\n",
    "    income = np.clip(rng.lognormal(mean=10.5, sigma=0.8, size=n_users), 20_000, 300_000)\n",
    "\n",
    "    # ── Behaviour ──────────────────────────────────────────────────────────────\n",
    "    sessions_per_month   = rng.poisson(lam=8,  size=n_users) + 1\n",
    "    avg_session_duration = rng.exponential(scale=12.0, size=n_users) + 2.0          # minutes\n",
    "    purchases_last_year  = rng.negative_binomial(n=2, p=0.3, size=n_users)\n",
    "    avg_order_value      = rng.gamma(shape=2.0, scale=30.0, size=n_users) + 10.0\n",
    "\n",
    "    # ── Device & Subscription Flags ────────────────────────────────────────────\n",
    "    is_mobile       = rng.binomial(n=1, p=0.65, size=n_users)\n",
    "    is_premium_user = rng.binomial(n=1, p=0.15, size=n_users)\n",
    "\n",
    "    # ── Region (one-hot) ───────────────────────────────────────────────────────\n",
    "    regions = rng.choice(['NA', 'EU', 'ASIA', 'OTHER'],\n",
    "                         size=n_users,\n",
    "                         p=[0.40, 0.30, 0.20, 0.10])\n",
    "    region_dummies = pd.get_dummies(regions, prefix='region')\n",
    "\n",
    "    # ── Assemble final DataFrame ───────────────────────────────────────────────\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": np.arange(n_users),\n",
    "        \"age\": age,\n",
    "        \"income\": income,\n",
    "        \"sessions_per_month\": sessions_per_month,\n",
    "        \"avg_session_duration\": avg_session_duration,\n",
    "        \"purchases_last_year\": purchases_last_year,\n",
    "        \"avg_order_value\": avg_order_value,\n",
    "        \"is_mobile\": is_mobile,\n",
    "        \"is_premium_user\": is_premium_user\n",
    "    }).join(region_dummies)\n",
    "\n",
    "    return df\n",
    "\n",
    "user_data = generate_user_data(1_000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating User Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User similarity matrix shape: (1000, 1000)\n",
      "Matrix properties:\n",
      "  - Symmetric: True\n",
      "  - Positive definite: True\n",
      "  - Diagonal mean: 1.0000\n",
      "  - Off-diagonal mean: 0.1732\n"
     ]
    }
   ],
   "source": [
    "def create_user_similarity_matrix(user_features, method='rbf', gamma=0.1):\n",
    "    \"\"\"\n",
    "    Create user similarity matrix based on features\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(user_features)\n",
    "    \n",
    "    if method == 'rbf':\n",
    "        # RBF kernel similarity\n",
    "        from sklearn.metrics.pairwise import rbf_kernel\n",
    "        K = rbf_kernel(features_scaled, gamma=gamma)\n",
    "    elif method == 'linear':\n",
    "        # Linear kernel\n",
    "        K = np.dot(features_scaled, features_scaled.T) / features_scaled.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'rbf' or 'linear'\")\n",
    "    \n",
    "    # Ensure positive definiteness\n",
    "    K += np.eye(K.shape[0]) * 1e-6\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Select numerical features for similarity calculation\n",
    "feature_cols = ['age', 'income', 'sessions_per_month', 'avg_session_duration',\n",
    "                'purchases_last_year', 'avg_order_value', 'is_mobile', \n",
    "                'is_premium_user', 'region_NA', 'region_EU', 'region_ASIA']\n",
    "\n",
    "user_features = user_data[feature_cols].values\n",
    "\n",
    "# Create similarity matrix\n",
    "K_users = create_user_similarity_matrix(user_features, method='rbf', gamma=0.1)\n",
    "\n",
    "print(f\"User similarity matrix shape: {K_users.shape}\")\n",
    "print(f\"Matrix properties:\")\n",
    "print(f\"  - Symmetric: {np.allclose(K_users, K_users.T)}\")\n",
    "print(f\"  - Positive definite: {np.all(np.linalg.eigvals(K_users) > 0)}\")\n",
    "print(f\"  - Diagonal mean: {np.mean(np.diag(K_users)):.4f}\")\n",
    "print(f\"  - Off-diagonal mean: {np.mean(K_users[~np.eye(K_users.shape[0], dtype=bool)]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Simulating A/B Test Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated A/B test outcomes:\n",
      "   user_id  n_sessions  control_conversions  treatment_conversions  \\\n",
      "0        0           3                    2                      2   \n",
      "1        1           8                    8                      8   \n",
      "2        2           3                    3                      3   \n",
      "3        3           4                    2                      4   \n",
      "4        4          11                    3                      4   \n",
      "\n",
      "   control_rate  treatment_rate  true_user_effect  \n",
      "0      0.666667        0.666667         -0.064119  \n",
      "1      1.000000        1.000000          0.169838  \n",
      "2      1.000000        1.000000          0.140043  \n",
      "3      0.500000        1.000000          0.321888  \n",
      "4      0.272727        0.363636          0.194040  \n",
      "\n",
      "True treatment effect statistics:\n",
      "  - Mean control rate: 0.7466\n",
      "  - Mean treatment rate: 0.7698\n",
      "  - True lift: 0.031\n"
     ]
    }
   ],
   "source": [
    "def simulate_ab_test_outcomes(user_data, user_similarity_matrix, treatment_effect=0.05):\n",
    "    \"\"\"\n",
    "    Simulate A/B test outcomes with realistic user heterogeneity\n",
    "    \"\"\"\n",
    "    n_users = len(user_data)\n",
    "    \n",
    "    # Base conversion probability depends on user characteristics\n",
    "    base_logits = (\n",
    "        -2.5 +  # Base intercept (low conversion rate)\n",
    "        0.0001 * user_data['income'] +  # Higher income -> higher conversion\n",
    "        0.05 * user_data['is_premium_user'] +  # Premium users convert more\n",
    "        0.02 * user_data['purchases_last_year'] +  # Purchase history matters\n",
    "        -0.03 * user_data['is_mobile']  # Mobile conversion slightly lower\n",
    "    )\n",
    "    \n",
    "    # Add correlated random user effects\n",
    "    L = np.linalg.cholesky(user_similarity_matrix + np.eye(n_users) * 0.01)\n",
    "    user_effects = L @ np.random.normal(0, 0.3, n_users)  # σ_u = 0.3\n",
    "    \n",
    "    # Control group conversion probabilities\n",
    "    control_logits = base_logits + user_effects\n",
    "    control_probs = 1 / (1 + np.exp(-control_logits))\n",
    "    \n",
    "    # Treatment effect (multiplicative on odds)\n",
    "    treatment_logits = control_logits + np.log(1 + treatment_effect)\n",
    "    treatment_probs = 1 / (1 + np.exp(-treatment_logits))\n",
    "    \n",
    "    # Simulate conversion outcomes\n",
    "    n_sessions_per_user = np.random.poisson(5, n_users) + 1  # 1-10 sessions\n",
    "    \n",
    "    outcomes = []\n",
    "    for i in range(n_users):\n",
    "        n_sessions = n_sessions_per_user[i]\n",
    "        control_conversions = np.random.binomial(n_sessions, control_probs[i])\n",
    "        treatment_conversions = np.random.binomial(n_sessions, treatment_probs[i])\n",
    "        \n",
    "        outcomes.append({\n",
    "            'user_id': i,\n",
    "            'n_sessions': n_sessions,\n",
    "            'control_conversions': control_conversions,\n",
    "            'treatment_conversions': treatment_conversions,\n",
    "            'control_rate': control_conversions / n_sessions,\n",
    "            'treatment_rate': treatment_conversions / n_sessions,\n",
    "            'true_user_effect': user_effects[i]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(outcomes)\n",
    "\n",
    "# Simulate outcomes\n",
    "outcomes_df = simulate_ab_test_outcomes(user_data, K_users, treatment_effect=0.15)\n",
    "\n",
    "print(\"Simulated A/B test outcomes:\")\n",
    "print(outcomes_df.head())\n",
    "print(f\"\\nTrue treatment effect statistics:\")\n",
    "print(f\"  - Mean control rate: {outcomes_df['control_rate'].mean():.4f}\")\n",
    "print(f\"  - Mean treatment rate: {outcomes_df['treatment_rate'].mean():.4f}\")\n",
    "print(f\"  - True lift: {(outcomes_df['treatment_rate'].mean() / outcomes_df['control_rate'].mean() - 1):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Optimal User Selection with CDMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing selection of 100 users from 1000 total users...\n",
      "Using CDMean criterion with λ = 0.1\n",
      "Starting TrainSelPy optimization\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 100000000 into shape (100,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_users, result\u001b[38;5;241m.\u001b[39mfitness\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run CDMean optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m optimal_users_cdmean, cdmean_value \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_user_selection_cdmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36moptimize_user_selection_cdmean\u001b[0;34m(n_select, variance_ratio)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing CDMean criterion with λ = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariance_ratio\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muser_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msetsizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mn_select\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msettypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUOS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcdmean_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m selected_users \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mselected_indices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimization completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/core.py:466\u001b[0m, in \u001b[0;36mtrain_sel\u001b[0;34m(data, candidates, setsizes, ntotal, settypes, stat, n_stat, target, control, init_sol, packages, n_jobs, verbose, solution_diversity)\u001b[0m\n\u001b[1;32m    452\u001b[0m         result \u001b[38;5;241m=\u001b[39m genetic_algorithm(\n\u001b[1;32m    453\u001b[0m             data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    454\u001b[0m             candidates\u001b[38;5;241m=\u001b[39mcandidates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m             solution_diversity\u001b[38;5;241m=\u001b[39msolution_diversity_param\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[43msetsizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetsizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43msettypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstat_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[43minit_sol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_sol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43msolution_diversity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolution_diversity_param\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# Island model optimization.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     inner_control \u001b[38;5;241m=\u001b[39m control\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/genetic_algorithm.py:788\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(data, candidates, setsizes, settypes, stat_func, target, control, init_sol, n_stat, is_parallel, solution_diversity)\u001b[0m\n\u001b[1;32m    785\u001b[0m             population[i]\u001b[38;5;241m.\u001b[39mdbl_values \u001b[38;5;241m=\u001b[39m [init_sol[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolnDBLMat\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# Evaluate initial population\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m \u001b[43mevaluate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_stat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Keep track of the best solution and fitness history\u001b[39;00m\n\u001b[1;32m    791\u001b[0m best_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(population, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mfitness)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/genetic_algorithm.py:182\u001b[0m, in \u001b[0;36mevaluate_fitness\u001b[0;34m(population, stat_func, data, n_stat, is_parallel, control)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Only integer or only double variables\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_int:\n\u001b[0;32m--> 182\u001b[0m         sol\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m=\u001b[39m \u001b[43mstat_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m         sol\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m=\u001b[39m stat_func(sol\u001b[38;5;241m.\u001b[39mdbl_values[\u001b[38;5;241m0\u001b[39m], data)\n",
      "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/optimization_criteria.py:135\u001b[0m, in \u001b[0;36mcdmean_opt\u001b[0;34m(soln, data)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Compute V_inv_2 (the second term) - FIXED the reshape issue\u001b[39;00m\n\u001b[1;32m    134\u001b[0m V_inv_flat \u001b[38;5;241m=\u001b[39m V_inv\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 135\u001b[0m V_inv_2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_inv_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_inv_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoln\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoln\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m sum_V_inv\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Compute the complete matrix\u001b[39;00m\n\u001b[1;32m    138\u001b[0m outmat \u001b[38;5;241m=\u001b[39m G_all_soln \u001b[38;5;241m@\u001b[39m (V_inv \u001b[38;5;241m-\u001b[39m V_inv_2) \u001b[38;5;241m@\u001b[39m G_all_soln\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 100000000 into shape (100,100)"
     ]
    }
   ],
   "source": [
    "def optimize_user_selection_cdmean(n_select=100, variance_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Optimize user selection for A/B test using CDMean criterion\n",
    "    \"\"\"\n",
    "    # Prepare data for TrainSelPy\n",
    "    ts_data = make_data(K=K_users)\n",
    "    ts_data[\"G\"] = K_users\n",
    "    ts_data[\"lambda\"] = variance_ratio  # σ²_e / σ²_u\n",
    "    \n",
    "    # Set up optimization\n",
    "    from trainselpy.core import set_control_default\n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 100  # Reduced for demo\n",
    "    control[\"npop\"] = 200\n",
    "    control[\"progress\"] = True\n",
    "    \n",
    "    print(f\"Optimizing selection of {n_select} users from {len(user_data)} total users...\")\n",
    "    print(f\"Using CDMean criterion with λ = {variance_ratio}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=cdmean_opt,\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    selected_users = result.selected_indices[0]\n",
    "    \n",
    "    print(f\"\\nOptimization completed!\")\n",
    "    print(f\"Final CDMean value: {result.fitness:.6f}\")\n",
    "    print(f\"Selected users: {selected_users[:10]}... (showing first 10)\")\n",
    "    \n",
    "    return selected_users, result.fitness\n",
    "\n",
    "# Run CDMean optimization\n",
    "optimal_users_cdmean, cdmean_value = optimize_user_selection_cdmean(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Optimal User Selection with PEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_user_selection_pev(n_select=100):\n",
    "    \"\"\"\n",
    "    Optimize user selection for A/B test using PEV criterion\n",
    "    \"\"\"\n",
    "    # For PEV optimization, we need to specify target users (those we want to predict)\n",
    "    # Let's use all non-selected users as targets\n",
    "    \n",
    "    def pev_objective(selected_indices, data):\n",
    "        \"\"\"Custom PEV objective for user selection\"\"\"\n",
    "        K = data[\"G\"]\n",
    "        lambda_val = data[\"lambda\"]\n",
    "        n_total = K.shape[0]\n",
    "        \n",
    "        # Target users are all non-selected users\n",
    "        target_indices = [i for i in range(n_total) if i not in selected_indices]\n",
    "        \n",
    "        if len(target_indices) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate PEV for target users\n",
    "        K_TT = K[np.ix_(selected_indices, selected_indices)]\n",
    "        K_PT = K[np.ix_(target_indices, selected_indices)]\n",
    "        K_PP_diag = np.diag(K)[target_indices]\n",
    "        \n",
    "        # Add lambda to diagonal for numerical stability\n",
    "        K_TT_inv = np.linalg.inv(K_TT + lambda_val * np.eye(len(selected_indices)))\n",
    "        \n",
    "        # Calculate PEV for each target user\n",
    "        pev_values = []\n",
    "        for i, target_idx in enumerate(target_indices):\n",
    "            k_i = K_PT[i:i+1, :]  # Row vector\n",
    "            pev_i = K_PP_diag[i] - k_i @ K_TT_inv @ k_i.T\n",
    "            pev_values.append(pev_i[0, 0])\n",
    "        \n",
    "        # Return negative mean PEV (since we want to minimize PEV)\n",
    "        mean_pev = np.mean(pev_values)\n",
    "        return -mean_pev\n",
    "    \n",
    "    # Prepare data\n",
    "    ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "    \n",
    "    # Set up optimization\n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 100\n",
    "    control[\"npop\"] = 200\n",
    "    control[\"progress\"] = True\n",
    "    \n",
    "    print(f\"Optimizing selection of {n_select} users using PEV criterion...\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=pev_objective,\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    selected_users = result.selected_indices[0]\n",
    "    \n",
    "    print(f\"\\nPEV optimization completed!\")\n",
    "    print(f\"Final objective value: {result.fitness:.6f}\")\n",
    "    print(f\"Selected users: {selected_users[:10]}... (showing first 10)\")\n",
    "    \n",
    "    return selected_users, -result.fitness  # Convert back to positive PEV\n",
    "\n",
    "# Run PEV optimization\n",
    "optimal_users_pev, pev_value = optimize_user_selection_pev(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Comparison with Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running random selection trials...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 100000000 into shape (100,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df, random_cdmeans, random_pevs\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Run comparison\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m comparison_results, random_cdmeans, random_pevs \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_selection_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mcompare_selection_methods\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate CDMean for random selection\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ts_data_random \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m: K_users, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m}\n\u001b[0;32m---> 18\u001b[0m cdmean_random \u001b[38;5;241m=\u001b[39m \u001b[43mcdmean_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_users\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_data_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m random_cdmeans\u001b[38;5;241m.\u001b[39mappend(cdmean_random)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate mean PEV for random selection\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/dakdemirGithub/GitHubProjects/trainselpy/trainselpy/optimization_criteria.py:135\u001b[0m, in \u001b[0;36mcdmean_opt\u001b[0;34m(soln, data)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Compute V_inv_2 (the second term) - FIXED the reshape issue\u001b[39;00m\n\u001b[1;32m    134\u001b[0m V_inv_flat \u001b[38;5;241m=\u001b[39m V_inv\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 135\u001b[0m V_inv_2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_inv_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_inv_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoln\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msoln\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m sum_V_inv\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Compute the complete matrix\u001b[39;00m\n\u001b[1;32m    138\u001b[0m outmat \u001b[38;5;241m=\u001b[39m G_all_soln \u001b[38;5;241m@\u001b[39m (V_inv \u001b[38;5;241m-\u001b[39m V_inv_2) \u001b[38;5;241m@\u001b[39m G_all_soln\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 100000000 into shape (100,100)"
     ]
    }
   ],
   "source": [
    "def compare_selection_methods():\n",
    "    \"\"\"\n",
    "    Compare optimal selection methods with random selection\n",
    "    \"\"\"\n",
    "    n_select = 100\n",
    "    n_random_trials = 50\n",
    "    \n",
    "    # Random selection baselines\n",
    "    random_cdmeans = []\n",
    "    random_pevs = []\n",
    "    \n",
    "    print(\"Running random selection trials...\")\n",
    "    for trial in range(n_random_trials):\n",
    "        random_users = np.random.choice(len(user_data), size=n_select, replace=False)\n",
    "        \n",
    "        # Calculate CDMean for random selection\n",
    "        ts_data_random = {\"G\": K_users, \"lambda\": 0.1}\n",
    "        cdmean_random = cdmean_opt(random_users.tolist(), ts_data_random)\n",
    "        random_cdmeans.append(cdmean_random)\n",
    "        \n",
    "        # Calculate mean PEV for random selection\n",
    "        non_selected = [i for i in range(len(user_data)) if i not in random_users]\n",
    "        K_TT = K_users[np.ix_(random_users, random_users)]\n",
    "        K_PT = K_users[np.ix_(non_selected, random_users)]\n",
    "        K_PP_diag = np.diag(K_users)[non_selected]\n",
    "        \n",
    "        K_TT_inv = np.linalg.inv(K_TT + 0.1 * np.eye(len(random_users)))\n",
    "        \n",
    "        pev_values = []\n",
    "        for i in range(len(non_selected)):\n",
    "            k_i = K_PT[i:i+1, :]\n",
    "            pev_i = K_PP_diag[i] - k_i @ K_TT_inv @ k_i.T\n",
    "            pev_values.append(pev_i[0, 0])\n",
    "        \n",
    "        random_pevs.append(np.mean(pev_values))\n",
    "    \n",
    "    # Create comparison results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Method': ['Random (mean)', 'Random (std)', 'CDMean Optimal', 'PEV Optimal'],\n",
    "        'CDMean': [\n",
    "            np.mean(random_cdmeans),\n",
    "            np.std(random_cdmeans),\n",
    "            cdmean_value,\n",
    "            np.nan  # We didn't calculate CDMean for PEV optimal\n",
    "        ],\n",
    "        'Mean PEV': [\n",
    "            np.mean(random_pevs),\n",
    "            np.std(random_pevs),\n",
    "            np.nan,  # We didn't calculate PEV for CDMean optimal\n",
    "            pev_value\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    cdmean_improvement = (cdmean_value - np.mean(random_cdmeans)) / np.mean(random_cdmeans) * 100\n",
    "    pev_improvement = (np.mean(random_pevs) - pev_value) / np.mean(random_pevs) * 100\n",
    "    \n",
    "    print(f\"\\nImprovements over random selection:\")\n",
    "    print(f\"  - CDMean optimization: {cdmean_improvement:.2f}% improvement\")\n",
    "    print(f\"  - PEV optimization: {pev_improvement:.2f}% improvement (lower is better)\")\n",
    "    \n",
    "    return results_df, random_cdmeans, random_pevs\n",
    "\n",
    "# Run comparison\n",
    "comparison_results, random_cdmeans, random_pevs = compare_selection_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_selection_results():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of selection results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. User characteristics comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Get characteristics for different selection methods\n",
    "    random_sample = np.random.choice(len(user_data), size=100, replace=False)\n",
    "    \n",
    "    methods = ['Random', 'CDMean Optimal', 'PEV Optimal']\n",
    "    selections = [random_sample, optimal_users_cdmean, optimal_users_pev]\n",
    "    colors = ['gray', 'blue', 'red']\n",
    "    \n",
    "    for i, (method, selection, color) in enumerate(zip(methods, selections, colors)):\n",
    "        selected_ages = user_data.iloc[selection]['age']\n",
    "        ax1.hist(selected_ages, alpha=0.6, label=method, color=color, bins=20)\n",
    "    \n",
    "    ax1.set_xlabel('Age')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Age Distribution by Selection Method')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Income distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    for i, (method, selection, color) in enumerate(zip(methods, selections, colors)):\n",
    "        selected_income = user_data.iloc[selection]['income']\n",
    "        ax2.hist(selected_income, alpha=0.6, label=method, color=color, bins=20)\n",
    "    \n",
    "    ax2.set_xlabel('Income')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Income Distribution by Selection Method')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. CDMean comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.hist(random_cdmeans, alpha=0.7, color='gray', bins=20, label='Random Selection')\n",
    "    ax3.axvline(cdmean_value, color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'CDMean Optimal: {cdmean_value:.6f}')\n",
    "    ax3.set_xlabel('CDMean Value')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('CDMean Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. PEV comparison\n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.hist(random_pevs, alpha=0.7, color='gray', bins=20, label='Random Selection')\n",
    "    ax4.axvline(pev_value, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'PEV Optimal: {pev_value:.6f}')\n",
    "    ax4.set_xlabel('Mean PEV Value')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Mean PEV Distribution')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Feature importance for optimal selections\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Calculate mean feature values for each selection method\n",
    "    feature_names = ['Age', 'Income', 'Sessions/Month', 'Session Duration', \n",
    "                     'Purchases', 'Order Value', 'Mobile', 'Premium']\n",
    "    feature_cols_viz = ['age', 'income', 'sessions_per_month', 'avg_session_duration',\n",
    "                        'purchases_last_year', 'avg_order_value', 'is_mobile', 'is_premium_user']\n",
    "    \n",
    "    # Normalize features for comparison\n",
    "    scaler = StandardScaler()\n",
    "    all_features_norm = scaler.fit_transform(user_data[feature_cols_viz])\n",
    "    \n",
    "    random_means = np.mean(all_features_norm[random_sample], axis=0)\n",
    "    cdmean_means = np.mean(all_features_norm[optimal_users_cdmean], axis=0)\n",
    "    pev_means = np.mean(all_features_norm[optimal_users_pev], axis=0)\n",
    "    \n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax5.bar(x - width, random_means, width, label='Random', color='gray', alpha=0.7)\n",
    "    ax5.bar(x, cdmean_means, width, label='CDMean Optimal', color='blue', alpha=0.7)\n",
    "    ax5.bar(x + width, pev_means, width, label='PEV Optimal', color='red', alpha=0.7)\n",
    "    \n",
    "    ax5.set_xlabel('Features')\n",
    "    ax5.set_ylabel('Normalized Mean Value')\n",
    "    ax5.set_title('Feature Profiles by Selection Method')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(feature_names, rotation=45)\n",
    "    ax5.legend()\n",
    "    \n",
    "    # 6. User similarity heatmap\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Sample small subset for visualization\n",
    "    viz_sample = np.random.choice(len(user_data), size=50, replace=False)\n",
    "    K_viz = K_users[np.ix_(viz_sample, viz_sample)]\n",
    "    \n",
    "    im = ax6.imshow(K_viz, cmap='viridis', aspect='auto')\n",
    "    ax6.set_title('User Similarity Matrix (Sample)')\n",
    "    ax6.set_xlabel('User Index')\n",
    "    ax6.set_ylabel('User Index')\n",
    "    plt.colorbar(im, ax=ax6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Random selection CDMean: {np.mean(random_cdmeans):.6f} ± {np.std(random_cdmeans):.6f}\")\n",
    "    print(f\"Optimal CDMean: {cdmean_value:.6f} (improvement: {((cdmean_value - np.mean(random_cdmeans)) / np.mean(random_cdmeans) * 100):.2f}%)\")\n",
    "    print(f\"Random selection PEV: {np.mean(random_pevs):.6f} ± {np.std(random_pevs):.6f}\")\n",
    "    print(f\"Optimal PEV: {pev_value:.6f} (improvement: {((np.mean(random_pevs) - pev_value) / np.mean(random_pevs) * 100):.2f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_selection_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Applications\n",
    "\n",
    "### 6.1 Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_objective_user_selection(n_select=100):\n",
    "    \"\"\"\n",
    "    Multi-objective optimization balancing CDMean, cost, and diversity\n",
    "    \"\"\"\n",
    "    def multi_objective_fitness(selected_indices, data):\n",
    "        \"\"\"\n",
    "        Combine multiple objectives:\n",
    "        1. CDMean (prediction accuracy)\n",
    "        2. Cost (based on user characteristics)\n",
    "        3. Diversity (demographic balance)\n",
    "        \"\"\"\n",
    "        # Objective 1: CDMean\n",
    "        cdmean_val = cdmean_opt(selected_indices, data)\n",
    "        \n",
    "        # Objective 2: Cost (higher income users are more expensive to recruit)\n",
    "        selected_users_df = user_data.iloc[selected_indices]\n",
    "        avg_income = selected_users_df['income'].mean()\n",
    "        cost_obj = -avg_income / 100000  # Negative because we want to minimize cost\n",
    "        \n",
    "        # Objective 3: Diversity (variance in key demographics)\n",
    "        age_diversity = selected_users_df['age'].std() / user_data['age'].std()\n",
    "        income_diversity = selected_users_df['income'].std() / user_data['income'].std()\n",
    "        diversity_obj = (age_diversity + income_diversity) / 2\n",
    "        \n",
    "        # Return multiple objectives\n",
    "        return [cdmean_val, cost_obj, diversity_obj]\n",
    "    \n",
    "    # Prepare data\n",
    "    ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "    \n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 50  # Reduced for multi-objective\n",
    "    control[\"npop\"] = 200\n",
    "    \n",
    "    print(\"Running multi-objective optimization...\")\n",
    "    print(\"Objectives: CDMean (max), Cost (min), Diversity (max)\")\n",
    "    \n",
    "    # Run multi-objective optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=multi_objective_fitness,\n",
    "        n_stat=3,  # Three objectives\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMulti-objective optimization completed!\")\n",
    "    print(f\"Found {len(result.pareto_front)} solutions on Pareto front\")\n",
    "    \n",
    "    # Analyze Pareto front\n",
    "    if result.pareto_front:\n",
    "        pareto_df = pd.DataFrame(result.pareto_front, \n",
    "                                columns=['CDMean', 'Cost (neg)', 'Diversity'])\n",
    "        pareto_df['Cost'] = -pareto_df['Cost (neg)']  # Convert back to positive\n",
    "        \n",
    "        print(\"\\nPareto Front Solutions:\")\n",
    "        print(pareto_df.describe())\n",
    "        \n",
    "        # Select compromise solution (closest to ideal point)\n",
    "        # Normalize objectives to [0,1] range\n",
    "        normalized_objectives = (pareto_df[['CDMean', 'Diversity']] - pareto_df[['CDMean', 'Diversity']].min()) / (pareto_df[['CDMean', 'Diversity']].max() - pareto_df[['CDMean', 'Diversity']].min())\n",
    "        normalized_cost = (pareto_df['Cost'].max() - pareto_df['Cost']) / (pareto_df['Cost'].max() - pareto_df['Cost'].min())\n",
    "        \n",
    "        # Ideal point: maximize CDMean and Diversity, minimize Cost\n",
    "        ideal_point = np.array([1.0, 1.0, 1.0])  # All objectives at their best\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(normalized_objectives)):\n",
    "            point = np.array([normalized_objectives.iloc[i]['CDMean'], \n",
    "                             normalized_cost.iloc[i], \n",
    "                             normalized_objectives.iloc[i]['Diversity']])\n",
    "            distance = np.linalg.norm(point - ideal_point)\n",
    "            distances.append(distance)\n",
    "        \n",
    "        best_compromise_idx = np.argmin(distances)\n",
    "        best_solution = result.pareto_solutions[best_compromise_idx]\n",
    "        \n",
    "        print(f\"\\nBest compromise solution (index {best_compromise_idx}):\")\n",
    "        print(f\"  - CDMean: {result.pareto_front[best_compromise_idx][0]:.6f}\")\n",
    "        print(f\"  - Cost: {-result.pareto_front[best_compromise_idx][1]:.0f}\")\n",
    "        print(f\"  - Diversity: {result.pareto_front[best_compromise_idx][2]:.4f}\")\n",
    "        \n",
    "        return result, best_solution['selected_indices'][0]\n",
    "    \n",
    "    return result, None\n",
    "\n",
    "# Run multi-objective optimization\n",
    "mo_result, best_compromise_users = multi_objective_user_selection(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Adaptive Experiment Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_experiment_design(initial_users=50, additional_batches=3, batch_size=25):\n",
    "    \"\"\"\n",
    "    Demonstrate adaptive experiment design where we sequentially add users\n",
    "    based on current results\n",
    "    \"\"\"\n",
    "    print(\"Adaptive Experiment Design Simulation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Start with initial random selection\n",
    "    current_users = np.random.choice(len(user_data), size=initial_users, replace=False).tolist()\n",
    "    \n",
    "    adaptation_history = []\n",
    "    \n",
    "    for batch in range(additional_batches):\n",
    "        print(f\"\\nBatch {batch + 1}: Adding {batch_size} more users\")\n",
    "        \n",
    "        # Calculate current CDMean\n",
    "        ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "        current_cdmean = cdmean_opt(current_users, ts_data)\n",
    "        \n",
    "        print(f\"Current CDMean with {len(current_users)} users: {current_cdmean:.6f}\")\n",
    "        \n",
    "        # Find remaining candidates\n",
    "        remaining_candidates = [i for i in range(len(user_data)) if i not in current_users]\n",
    "        \n",
    "        # Optimize selection of additional users\n",
    "        def incremental_fitness(additional_indices, data):\n",
    "            combined_users = current_users + additional_indices\n",
    "            return cdmean_opt(combined_users, data)\n",
    "        \n",
    "        control = set_control_default()\n",
    "        control[\"niterations\"] = 30\n",
    "        control[\"npop\"] = 100\n",
    "        \n",
    "        result = train_sel(\n",
    "            data=ts_data,\n",
    "            candidates=[remaining_candidates],\n",
    "            setsizes=[batch_size],\n",
    "            settypes=[\"UOS\"],\n",
    "            stat=incremental_fitness,\n",
    "            control=control,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Add selected users\n",
    "        new_users = result.selected_indices[0]\n",
    "        current_users.extend(new_users)\n",
    "        \n",
    "        new_cdmean = result.fitness\n",
    "        improvement = new_cdmean - current_cdmean\n",
    "        \n",
    "        print(f\"Added users: {new_users[:5]}... (showing first 5)\")\n",
    "        print(f\"New CDMean: {new_cdmean:.6f}\")\n",
    "        print(f\"Improvement: {improvement:.6f} ({improvement/current_cdmean*100:.2f}%)\")\n",
    "        \n",
    "        adaptation_history.append({\n",
    "            'batch': batch + 1,\n",
    "            'total_users': len(current_users),\n",
    "            'cdmean': new_cdmean,\n",
    "            'improvement': improvement,\n",
    "            'new_users': new_users\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nFinal adaptive selection:\")\n",
    "    print(f\"  - Total users: {len(current_users)}\")\n",
    "    print(f\"  - Final CDMean: {adaptation_history[-1]['cdmean']:.6f}\")\n",
    "    print(f\"  - Total improvement: {adaptation_history[-1]['cdmean'] - cdmean_opt(current_users[:initial_users], ts_data):.6f}\")\n",
    "    \n",
    "    return current_users, adaptation_history\n",
    "\n",
    "# Run adaptive design\n",
    "adaptive_users, adaptation_history = adaptive_experiment_design()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Business Impact Analysis\n",
    "\n",
    "### 7.1 Statistical Power Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis_comparison():\n",
    "    \"\"\"\n",
    "    Compare statistical power between optimal and random user selection\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Simulate A/B test results for different selection methods\n",
    "    methods = {\n",
    "        'Random': np.random.choice(len(user_data), size=100, replace=False),\n",
    "        'CDMean Optimal': optimal_users_cdmean,\n",
    "        'PEV Optimal': optimal_users_pev\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for method_name, selected_users in methods.items():\n",
    "        # Get outcomes for selected users\n",
    "        selected_outcomes = outcomes_df.iloc[selected_users]\n",
    "        \n",
    "        # Calculate treatment effect\n",
    "        control_rate = selected_outcomes['control_rate'].mean()\n",
    "        treatment_rate = selected_outcomes['treatment_rate'].mean()\n",
    "        observed_lift = (treatment_rate - control_rate) / control_rate\n",
    "        \n",
    "        # Statistical test\n",
    "        control_conversions = selected_outcomes['control_conversions'].sum()\n",
    "        control_sessions = selected_outcomes['n_sessions'].sum()\n",
    "        treatment_conversions = selected_outcomes['treatment_conversions'].sum()\n",
    "        treatment_sessions = selected_outcomes['n_sessions'].sum()\n",
    "        \n",
    "        # Two-proportion z-test\n",
    "        p_pooled = (control_conversions + treatment_conversions) / (control_sessions + treatment_sessions)\n",
    "        se_pooled = np.sqrt(p_pooled * (1 - p_pooled) * (1/control_sessions + 1/treatment_sessions))\n",
    "        z_stat = (treatment_rate - control_rate) / se_pooled\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        # Effect size (Cohen's h for proportions)\n",
    "        cohens_h = 2 * (np.arcsin(np.sqrt(treatment_rate)) - np.arcsin(np.sqrt(control_rate)))\n",
    "        \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Control Rate': control_rate,\n",
    "            'Treatment Rate': treatment_rate,\n",
    "            'Observed Lift': observed_lift,\n",
    "            'Z-statistic': z_stat,\n",
    "            'P-value': p_value,\n",
    "            'Significant (α=0.05)': p_value < 0.05,\n",
    "            'Cohen\\'s h': cohens_h,\n",
    "            'Sample Size': len(selected_users)\n",
    "        })\n",
    "    \n",
    "    power_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"Statistical Power Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(power_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Calculate required sample sizes for 80% power\n",
    "    print(\"\\nSample size requirements for 80% power:\")\n",
    "    alpha = 0.05\n",
    "    power = 0.80\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    for _, row in power_df.iterrows():\n",
    "        control_rate = row['Control Rate']\n",
    "        effect_size = row['Observed Lift']\n",
    "        treatment_rate = control_rate * (1 + effect_size)\n",
    "        \n",
    "        p_pooled = (control_rate + treatment_rate) / 2\n",
    "        required_n = ((z_alpha + z_beta) * np.sqrt(2 * p_pooled * (1 - p_pooled)) / (treatment_rate - control_rate))**2\n",
    "        \n",
    "        efficiency_gain = (required_n / row['Sample Size'] - 1) * 100 if row['Significant (α=0.05)'] else np.nan\n",
    "        \n",
    "        print(f\"  {row['Method']}: {required_n:.0f} users per group (efficiency gain: {efficiency_gain:.1f}%)\")\n",
    "    \n",
    "    return power_df\n",
    "\n",
    "# Run power analysis\n",
    "power_comparison = power_analysis_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_benefit_analysis():\n",
    "    \"\"\"\n",
    "    Estimate business value of optimal user selection\n",
    "    \"\"\"\n",
    "    # Assumptions\n",
    "    cost_per_user = 50  # Cost to include one user in experiment\n",
    "    revenue_per_conversion = 100  # Average revenue per conversion\n",
    "    total_traffic_per_month = 100000  # Total monthly users\n",
    "    \n",
    "    print(\"Cost-Benefit Analysis of Optimal User Selection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Random baseline\n",
    "    random_sample = np.random.choice(len(user_data), size=100, replace=False)\n",
    "    random_outcomes = outcomes_df.iloc[random_sample]\n",
    "    random_control_rate = random_outcomes['control_rate'].mean()\n",
    "    random_treatment_rate = random_outcomes['treatment_rate'].mean()\n",
    "    random_lift = (random_treatment_rate - random_control_rate) / random_control_rate\n",
    "    \n",
    "    # Optimal selection outcomes\n",
    "    optimal_outcomes = outcomes_df.iloc[optimal_users_cdmean]\n",
    "    optimal_control_rate = optimal_outcomes['control_rate'].mean()\n",
    "    optimal_treatment_rate = optimal_outcomes['treatment_rate'].mean()\n",
    "    optimal_lift = (optimal_treatment_rate - optimal_control_rate) / optimal_control_rate\n",
    "    \n",
    "    print(f\"Experiment costs:\")\n",
    "    print(f\"  - Cost per user in experiment: ${cost_per_user}\")\n",
    "    print(f\"  - Total experiment cost: ${100 * cost_per_user:,}\")\n",
    "    \n",
    "    print(f\"\\nObserved effects:\")\n",
    "    print(f\"  - Random selection lift: {random_lift:.3f} ({random_lift*100:.1f}%)\")\n",
    "    print(f\"  - Optimal selection lift: {optimal_lift:.3f} ({optimal_lift*100:.1f}%)\")\n",
    "    \n",
    "    # Assume the optimal selection gives a more accurate estimate of true population effect\n",
    "    # due to better representativeness (higher CDMean)\n",
    "    \n",
    "    # Simulate deployment to full population\n",
    "    monthly_conversions_control = total_traffic_per_month * random_control_rate\n",
    "    monthly_conversions_treatment_random = total_traffic_per_month * (random_control_rate * (1 + random_lift))\n",
    "    monthly_conversions_treatment_optimal = total_traffic_per_month * (random_control_rate * (1 + optimal_lift))\n",
    "    \n",
    "    # Revenue impact\n",
    "    monthly_revenue_baseline = monthly_conversions_control * revenue_per_conversion\n",
    "    monthly_revenue_improvement_random = (monthly_conversions_treatment_random - monthly_conversions_control) * revenue_per_conversion\n",
    "    monthly_revenue_improvement_optimal = (monthly_conversions_treatment_optimal - monthly_conversions_control) * revenue_per_conversion\n",
    "    \n",
    "    # The key insight: optimal selection provides more accurate estimate of true effect\n",
    "    # This reduces risk of deploying ineffective treatments or missing good ones\n",
    "    \n",
    "    accuracy_improvement = optimal_lift / random_lift if random_lift > 0 else 1\n",
    "    \n",
    "    print(f\"\\nProjected monthly impact (if treatment is deployed):\")\n",
    "    print(f\"  - Baseline monthly revenue: ${monthly_revenue_baseline:,.0f}\")\n",
    "    print(f\"  - Revenue improvement (random estimate): ${monthly_revenue_improvement_random:,.0f}\")\n",
    "    print(f\"  - Revenue improvement (optimal estimate): ${monthly_revenue_improvement_optimal:,.0f}\")\n",
    "    print(f\"  - Difference in estimates: ${abs(monthly_revenue_improvement_optimal - monthly_revenue_improvement_random):,.0f}\")\n",
    "    \n",
    "    # ROI calculation\n",
    "    experiment_cost = 100 * cost_per_user\n",
    "    monthly_value_difference = abs(monthly_revenue_improvement_optimal - monthly_revenue_improvement_random)\n",
    "    roi_months = experiment_cost / monthly_value_difference if monthly_value_difference > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\nReturn on Investment:\")\n",
    "    print(f\"  - Additional experiment cost for optimal selection: $0 (same sample size)\")\n",
    "    print(f\"  - Value of more accurate effect estimate: ${monthly_value_difference:,.0f}/month\")\n",
    "    print(f\"  - Break-even time: {roi_months:.1f} months\")\n",
    "    \n",
    "    # Risk reduction\n",
    "    print(f\"\\nRisk reduction benefits:\")\n",
    "    print(f\"  - Reduced risk of false positives/negatives\")\n",
    "    print(f\"  - Better external validity (CDMean: {cdmean_value:.4f} vs random mean: {np.mean(random_cdmeans):.4f})\")\n",
    "    print(f\"  - More confident business decisions\")\n",
    "    \n",
    "    return {\n",
    "        'experiment_cost': experiment_cost,\n",
    "        'monthly_value_difference': monthly_value_difference,\n",
    "        'roi_months': roi_months,\n",
    "        'accuracy_improvement': accuracy_improvement\n",
    "    }\n",
    "\n",
    "# Run cost-benefit analysis\n",
    "cost_benefit_results = cost_benefit_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Future Directions\n",
    "\n",
    "### 8.1 Key Findings\n",
    "\n",
    "Our analysis demonstrates several critical advantages of using TrainSelPy for A/B test design:\n",
    "\n",
    "1. **Improved Prediction Accuracy**: CDMean optimization achieved X% improvement over random selection\n",
    "2. **Reduced Prediction Uncertainty**: PEV optimization reduced prediction variance by Y%\n",
    "3. **Enhanced External Validity**: Optimal selection provides better generalization to target populations\n",
    "4. **Multi-objective Flexibility**: Can balance statistical power, cost, and diversity simultaneously\n",
    "\n",
    "### 8.2 Practical Implementation Guidelines\n",
    "\n",
    "1. **When to Use Optimal Selection**:\n",
    "   - Limited testing capacity (< 10% of user base)\n",
    "   - High-stakes experiments requiring maximum precision\n",
    "   - Need for strong external validity guarantees\n",
    "   - Multi-constraint optimization problems\n",
    "\n",
    "2. **Recommended Workflow**:\n",
    "   - Collect user characteristics and compute similarity matrix\n",
    "   - Define optimization objectives (CDMean, PEV, cost, diversity)\n",
    "   - Run TrainSelPy optimization with appropriate constraints\n",
    "   - Validate selection with pilot studies\n",
    "   - Deploy optimized experiment design\n",
    "\n",
    "3. **Key Considerations**:\n",
    "   - Quality of user similarity matrix is crucial\n",
    "   - Variance ratio (λ) estimation affects results\n",
    "   - Computational cost scales with problem size\n",
    "   - Results should be validated against business outcomes\n",
    "\n",
    "### 8.3 Future Research Directions\n",
    "\n",
    "1. **Dynamic Similarity Matrices**: Update user relationships based on real-time behavior\n",
    "2. **Causal Inference Integration**: Combine with doubly-robust estimation methods\n",
    "3. **Network Effect Modeling**: Extend to experiments with user interactions\n",
    "4. **Bayesian Optimization**: Incorporate prior beliefs about treatment effects\n",
    "5. **Multi-Armed Bandit Hybrid**: Combine optimal design with adaptive allocation\n",
    "\n",
    "### 8.4 Business Impact Summary\n",
    "\n",
    "The integration of mixed models and genetic algorithm optimization represents a paradigm shift from traditional A/B testing:\n",
    "\n",
    "- **From random sampling** → **strategic participant selection**\n",
    "- **From single objectives** → **multi-objective optimization**\n",
    "- **From fixed designs** → **adaptive experimentation**\n",
    "- **From internal validity focus** → **external validity optimization**\n",
    "\n",
    "This approach is particularly valuable for organizations with:\n",
    "- Large user bases requiring sample efficiency\n",
    "- Complex user heterogeneity patterns\n",
    "- Multiple competing business objectives\n",
    "- Strong needs for generalizability\n",
    "\n",
    "The mathematical rigor of mixed models combined with the optimization power of genetic algorithms opens new frontiers in experimental design, making A/B testing more scientific, efficient, and business-aligned.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Henderson, C.R. (1984). Applications of linear models in animal breeding. University of Guelph.\n",
    "2. Akdemir, D., et al. (2021). TrainSel: An R Package for Selection of Training Populations. *Frontiers in Genetics*, 12, 655287.\n",
    "3. Deng, A., et al. (2013). Improving the sensitivity of online controlled experiments by utilizing pre-experiment data. *Proceedings of WSDM*.\n",
    "4. Kohavi, R., et al. (2020). Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n",
    "5. Athey, S., & Imbens, G. W. (2017). The econometrics of randomized experiments. *Handbook of Economic Field Experiments*, 1, 73-140."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
