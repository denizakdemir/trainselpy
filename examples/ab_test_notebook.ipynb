{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal A/B Test Design Using Mixed Models and TrainSelPy\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook bridges classical experimental design optimization with modern A/B testing by formulating digital experiments as mixed-effects models and applying TrainSelPy's CDMean and PEV criteria for optimal participant selection. We demonstrate how genetic algorithm optimization can dramatically improve experimental efficiency and external validity compared to traditional random sampling approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### 1.1 Limitations of Traditional A/B Testing Design\n",
    "\n",
    "Traditional A/B testing relies on **simple random assignment** with the implicit assumption that:\n",
    "1. All users are equally informative\n",
    "2. Random sampling guarantees representativeness\n",
    "3. Larger samples always yield better estimates\n",
    "\n",
    "However, in practice:\n",
    "- **Testing capacity is limited** (infrastructure, ethics, cost)\n",
    "- **Users are heterogeneous** with varying response patterns  \n",
    "- **Network effects** create dependencies between users\n",
    "- **External validity** to broader populations is uncertain\n",
    "\n",
    "### 1.2 The Optimization Opportunity\n",
    "\n",
    "Instead of random sampling, we can **optimize participant selection** to:\n",
    "- Maximize statistical power with fewer participants\n",
    "- Improve generalizability to target populations\n",
    "- Balance multiple experimental objectives\n",
    "- Account for user heterogeneity and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mixed Models in A/B Testing\n",
    "\n",
    "### 2.1 Mathematical Formulation\n",
    "\n",
    "Digital experiments naturally exhibit **hierarchical structure**:\n",
    "\n",
    "$$Y_{ijk} = \\mu + \\tau_j + \\beta'X_{ijk} + u_i + \\epsilon_{ijk}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{ijk}$ = outcome for user $i$, treatment $j$, observation $k$\n",
    "- $\\mu$ = overall intercept\n",
    "- $\\tau_j$ = fixed treatment effect for treatment $j$\n",
    "- $\\beta'X_{ijk}$ = fixed effects of covariates\n",
    "- $u_i \\sim N(0, \\sigma_u^2)$ = random user effect\n",
    "- $\\epsilon_{ijk} \\sim N(0, \\sigma_e^2)$ = residual error\n",
    "\n",
    "### 2.2 Matrix Formulation\n",
    "\n",
    "In matrix form:\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ = $(n \\times 1)$ vector of responses\n",
    "- $\\mathbf{X}$ = $(n \\times p)$ design matrix for fixed effects\n",
    "- $\\mathbf{Z}$ = $(n \\times q)$ design matrix for random effects\n",
    "- $\\boldsymbol{\\beta}$ = $(p \\times 1)$ vector of fixed effects\n",
    "- $\\mathbf{u} \\sim N(\\mathbf{0}, \\mathbf{G})$ = $(q \\times 1)$ vector of random effects\n",
    "- $\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\mathbf{R})$ = $(n \\times 1)$ vector of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mixed Model Equations\n",
    "\n",
    "The **Best Linear Unbiased Predictors (BLUPs)** are obtained by solving:\n",
    "\n",
    "$$\\begin{bmatrix} \n",
    "\\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{Z} \\\\\n",
    "\\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{X} & \\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{Z} + \\mathbf{G}^{-1}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\hat{\\boldsymbol{\\beta}} \\\\ \n",
    "\\hat{\\mathbf{u}} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\mathbf{X}'\\mathbf{R}^{-1}\\mathbf{y} \\\\ \n",
    "\\mathbf{Z}'\\mathbf{R}^{-1}\\mathbf{y} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### 2.4 Why Mixed Models Matter in A/B Testing\n",
    "\n",
    "1. **User Heterogeneity**: Random user effects $u_i$ capture individual differences\n",
    "2. **Repeated Measurements**: Multiple sessions/actions per user create clustering\n",
    "3. **Generalization**: Mixed models explicitly model population-level effects\n",
    "4. **Efficiency**: Proper modeling of correlation structure improves precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coefficient of Determination Mean (CDMean)\n",
    "\n",
    "### 3.1 Mathematical Definition\n",
    "\n",
    "For a mixed model with training set $T$ and prediction set $P$, the **Coefficient of Determination (CD)** for individual $i$ is:\n",
    "\n",
    "$$CD_i = 1 - \\frac{PEV_i}{\\sigma_u^2}$$\n",
    "\n",
    "Where $PEV_i$ is the **Prediction Error Variance**:\n",
    "$$PEV_i = \\text{Var}(\\hat{u}_i - u_i | \\text{training data})$$\n",
    "\n",
    "The **CDMean** criterion is:\n",
    "$$\\text{CDMean} = \\frac{1}{|P|} \\sum_{i \\in P} CD_i = 1 - \\frac{1}{|P|} \\sum_{i \\in P} \\frac{PEV_i}{\\sigma_u^2}$$\n",
    "\n",
    "### 3.2 TrainSelPy Implementation\n",
    "\n",
    "The CDMean in TrainSelPy is computed as:\n",
    "\n",
    "$$\\text{CDMean} = \\frac{1}{n-|T|} \\sum_{i \\notin T} \\mathbf{g}_i' (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\left[\\mathbf{I} - \\frac{\\mathbf{1}\\mathbf{1}'}{1'\\mathbf{1}}\\right] (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\mathbf{g}_i / g_{ii}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{G}_{TT}$ = relationship matrix among training individuals\n",
    "- $\\mathbf{g}_i$ = relationship vector between individual $i$ and training set\n",
    "- $\\lambda = \\sigma_e^2/\\sigma_u^2$ = variance ratio\n",
    "- $g_{ii}$ = diagonal element of $\\mathbf{G}$ for individual $i$\n",
    "\n",
    "### 3.3 Interpretation in A/B Testing\n",
    "\n",
    "CDMean answers: **\"How well can our experimental participants predict treatment effects for non-participants?\"**\n",
    "\n",
    "- **CDMean = 1**: Perfect prediction (experimental group perfectly represents population)\n",
    "- **CDMean = 0**: No predictive value (experimental group uninformative)\n",
    "- **Higher CDMean**: Better external validity and generalizability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Error Variance (PEV)\n",
    "\n",
    "### 4.1 Mathematical Definition\n",
    "\n",
    "For individual $i$ not in the training set, the **PEV** is:\n",
    "\n",
    "$$PEV_i = \\text{Var}(\\hat{u}_i | \\text{training data}) = g_{ii} - \\mathbf{g}_i' (\\mathbf{G}_{TT} + \\lambda\\mathbf{I})^{-1} \\mathbf{g}_i$$\n",
    "\n",
    "The **mean PEV** criterion minimizes:\n",
    "$$\\text{Mean PEV} = \\frac{1}{|P|} \\sum_{i \\in P} PEV_i$$\n",
    "\n",
    "### 4.2 Interpretation in A/B Testing\n",
    "\n",
    "PEV answers: **\"How uncertain are our predictions for non-participants?\"**\n",
    "\n",
    "- **Lower PEV**: More precise predictions for population\n",
    "- **Minimizing PEV**: Optimal selection for reducing prediction uncertainty\n",
    "- **Direct connection**: To confidence intervals for treatment effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Implementation with TrainSelPy\n",
    "\n",
    "### 5.1 Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/__init__.py:24\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/multiarray.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/overrides.py:8\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[32m     12\u001b[39m ARRAY_FUNCTIONS = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib\n  Referenced from: <C435405B-BFF1-399F-A1EC-BD43418F546A> /Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libopenblas.0.dylib\n  Reason: tried: '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/__init__.py:130\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/__config__.py:4\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     __cpu_features__,\n\u001b[32m      6\u001b[39m     __cpu_baseline__,\n\u001b[32m      7\u001b[39m     __cpu_dispatch__,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/__init__.py:50\u001b[39m\n\u001b[32m     27\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[33m\"\"\"\u001b[39m % (sys.version_info[\u001b[32m0\u001b[39m], sys.version_info[\u001b[32m1\u001b[39m], sys.executable,\n\u001b[32m     49\u001b[39m         __version__, exc)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.12 from \"/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/bin/python\"\n  * The NumPy version is: \"1.26.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib\n  Referenced from: <C435405B-BFF1-399F-A1EC-BD43418F546A> /Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libopenblas.0.dylib\n  Reason: tried: '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/dakdemir/Library/r-miniconda-arm64/envs/ag/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache)\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/r-miniconda-arm64/envs/ag/lib/python3.12/site-packages/numpy/__init__.py:135\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    132\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\u001b[33mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[33m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[33m    your python interpreter from there.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    137\u001b[39m __all__ = [\n\u001b[32m    138\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mModuleDeprecationWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVisibleDeprecationWarning\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    139\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mComplexWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTooHardError\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAxisError\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# mapping of {name: (value, deprecation_msg)}\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from trainselpy import train_sel, make_data, cdmean_opt, pev_opt, dopt, set_control_default\n",
    "\n",
    "\n",
    "def generate_user_data(n_users: int = 1000, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic e-commerce user data with realistic marginal\n",
    "    distributions and one-hot region encodings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_users : int\n",
    "        Number of simulated users.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame comprising demographic, behavioural, device,\n",
    "        subscription and regional dummy variables.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # ── Demographics ───────────────────────────────────────────────────────────\n",
    "    age = np.clip(rng.gamma(shape=2.0, scale=15.0, size=n_users) + 18.0, 18, 80)\n",
    "    income = np.clip(rng.lognormal(mean=10.5, sigma=0.8, size=n_users), 20_000, 300_000)\n",
    "\n",
    "    # ── Behaviour ──────────────────────────────────────────────────────────────\n",
    "    sessions_per_month   = rng.poisson(lam=8,  size=n_users) + 1\n",
    "    avg_session_duration = rng.exponential(scale=12.0, size=n_users) + 2.0          # minutes\n",
    "    purchases_last_year  = rng.negative_binomial(n=2, p=0.3, size=n_users)\n",
    "    avg_order_value      = rng.gamma(shape=2.0, scale=30.0, size=n_users) + 10.0\n",
    "\n",
    "    # ── Device & Subscription Flags ────────────────────────────────────────────\n",
    "    is_mobile       = rng.binomial(n=1, p=0.65, size=n_users)\n",
    "    is_premium_user = rng.binomial(n=1, p=0.15, size=n_users)\n",
    "\n",
    "    # ── Region (one-hot) ───────────────────────────────────────────────────────\n",
    "    regions = rng.choice(['NA', 'EU', 'ASIA', 'OTHER'],\n",
    "                         size=n_users,\n",
    "                         p=[0.40, 0.30, 0.20, 0.10])\n",
    "    region_dummies = pd.get_dummies(regions, prefix='region')\n",
    "\n",
    "    # ── Assemble final DataFrame ───────────────────────────────────────────────\n",
    "    df = pd.DataFrame({\n",
    "        \"user_id\": np.arange(n_users),\n",
    "        \"age\": age,\n",
    "        \"income\": income,\n",
    "        \"sessions_per_month\": sessions_per_month,\n",
    "        \"avg_session_duration\": avg_session_duration,\n",
    "        \"purchases_last_year\": purchases_last_year,\n",
    "        \"avg_order_value\": avg_order_value,\n",
    "        \"is_mobile\": is_mobile,\n",
    "        \"is_premium_user\": is_premium_user\n",
    "    }).join(region_dummies)\n",
    "\n",
    "    return df\n",
    "\n",
    "user_data = generate_user_data(1_000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating User Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_similarity_matrix(user_features, method='rbf', gamma=0.1):\n",
    "    \"\"\"\n",
    "    Create user similarity matrix based on features\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(user_features)\n",
    "    \n",
    "    if method == 'rbf':\n",
    "        # RBF kernel similarity\n",
    "        from sklearn.metrics.pairwise import rbf_kernel\n",
    "        K = rbf_kernel(features_scaled, gamma=gamma)\n",
    "    elif method == 'linear':\n",
    "        # Linear kernel\n",
    "        K = np.dot(features_scaled, features_scaled.T) / features_scaled.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'rbf' or 'linear'\")\n",
    "    \n",
    "    # Ensure positive definiteness\n",
    "    K += np.eye(K.shape[0]) * 1e-6\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Select numerical features for similarity calculation\n",
    "feature_cols = ['age', 'income', 'sessions_per_month', 'avg_session_duration',\n",
    "                'purchases_last_year', 'avg_order_value', 'is_mobile', \n",
    "                'is_premium_user', 'region_NA', 'region_EU', 'region_ASIA']\n",
    "\n",
    "user_features = user_data[feature_cols].values\n",
    "\n",
    "# Create similarity matrix\n",
    "K_users = create_user_similarity_matrix(user_features, method='rbf', gamma=0.1)\n",
    "\n",
    "print(f\"User similarity matrix shape: {K_users.shape}\")\n",
    "print(f\"Matrix properties:\")\n",
    "print(f\"  - Symmetric: {np.allclose(K_users, K_users.T)}\")\n",
    "print(f\"  - Positive definite: {np.all(np.linalg.eigvals(K_users) > 0)}\")\n",
    "print(f\"  - Diagonal mean: {np.mean(np.diag(K_users)):.4f}\")\n",
    "print(f\"  - Off-diagonal mean: {np.mean(K_users[~np.eye(K_users.shape[0], dtype=bool)]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Simulating A/B Test Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ab_test_outcomes(user_data, user_similarity_matrix, treatment_effect=0.05):\n",
    "    \"\"\"\n",
    "    Simulate A/B test outcomes with realistic user heterogeneity\n",
    "    \"\"\"\n",
    "    n_users = len(user_data)\n",
    "    \n",
    "    # Base conversion probability depends on user characteristics\n",
    "    base_logits = (\n",
    "        -2.5 +  # Base intercept (low conversion rate)\n",
    "        0.0001 * user_data['income'] +  # Higher income -> higher conversion\n",
    "        0.05 * user_data['is_premium_user'] +  # Premium users convert more\n",
    "        0.02 * user_data['purchases_last_year'] +  # Purchase history matters\n",
    "        -0.03 * user_data['is_mobile']  # Mobile conversion slightly lower\n",
    "    )\n",
    "    \n",
    "    # Add correlated random user effects\n",
    "    L = np.linalg.cholesky(user_similarity_matrix + np.eye(n_users) * 0.01)\n",
    "    user_effects = L @ np.random.normal(0, 0.3, n_users)  # σ_u = 0.3\n",
    "    \n",
    "    # Control group conversion probabilities\n",
    "    control_logits = base_logits + user_effects\n",
    "    control_probs = 1 / (1 + np.exp(-control_logits))\n",
    "    \n",
    "    # Treatment effect (multiplicative on odds)\n",
    "    treatment_logits = control_logits + np.log(1 + treatment_effect)\n",
    "    treatment_probs = 1 / (1 + np.exp(-treatment_logits))\n",
    "    \n",
    "    # Simulate conversion outcomes\n",
    "    n_sessions_per_user = np.random.poisson(5, n_users) + 1  # 1-10 sessions\n",
    "    \n",
    "    outcomes = []\n",
    "    for i in range(n_users):\n",
    "        n_sessions = n_sessions_per_user[i]\n",
    "        control_conversions = np.random.binomial(n_sessions, control_probs[i])\n",
    "        treatment_conversions = np.random.binomial(n_sessions, treatment_probs[i])\n",
    "        \n",
    "        outcomes.append({\n",
    "            'user_id': i,\n",
    "            'n_sessions': n_sessions,\n",
    "            'control_conversions': control_conversions,\n",
    "            'treatment_conversions': treatment_conversions,\n",
    "            'control_rate': control_conversions / n_sessions,\n",
    "            'treatment_rate': treatment_conversions / n_sessions,\n",
    "            'true_user_effect': user_effects[i]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(outcomes)\n",
    "\n",
    "# Simulate outcomes\n",
    "outcomes_df = simulate_ab_test_outcomes(user_data, K_users, treatment_effect=0.15)\n",
    "\n",
    "print(\"Simulated A/B test outcomes:\")\n",
    "print(outcomes_df.head())\n",
    "print(f\"\\nTrue treatment effect statistics:\")\n",
    "print(f\"  - Mean control rate: {outcomes_df['control_rate'].mean():.4f}\")\n",
    "print(f\"  - Mean treatment rate: {outcomes_df['treatment_rate'].mean():.4f}\")\n",
    "print(f\"  - True lift: {(outcomes_df['treatment_rate'].mean() / outcomes_df['control_rate'].mean() - 1):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Optimal User Selection with CDMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_user_selection_cdmean(n_select=100, variance_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Optimize user selection for A/B test using CDMean criterion\n",
    "    \"\"\"\n",
    "    # Prepare data for TrainSelPy\n",
    "    ts_data = make_data(K=K_users)\n",
    "    ts_data[\"G\"] = K_users\n",
    "    ts_data[\"lambda\"] = variance_ratio  # σ²_e / σ²_u\n",
    "    \n",
    "    # Set up optimization\n",
    "    from trainselpy.core import set_control_default\n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 100  # Reduced for demo\n",
    "    control[\"npop\"] = 200\n",
    "    control[\"progress\"] = True\n",
    "    \n",
    "    print(f\"Optimizing selection of {n_select} users from {len(user_data)} total users...\")\n",
    "    print(f\"Using CDMean criterion with λ = {variance_ratio}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=cdmean_opt,\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    selected_users = result.selected_indices[0]\n",
    "    \n",
    "    print(f\"\\nOptimization completed!\")\n",
    "    print(f\"Final CDMean value: {result.fitness:.6f}\")\n",
    "    print(f\"Selected users: {selected_users[:10]}... (showing first 10)\")\n",
    "    \n",
    "    return selected_users, result.fitness\n",
    "\n",
    "# Run CDMean optimization\n",
    "optimal_users_cdmean, cdmean_value = optimize_user_selection_cdmean(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Optimal User Selection with PEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_user_selection_pev(n_select=100):\n",
    "    \"\"\"\n",
    "    Optimize user selection for A/B test using PEV criterion\n",
    "    \"\"\"\n",
    "    # For PEV optimization, we need to specify target users (those we want to predict)\n",
    "    # Let's use all non-selected users as targets\n",
    "    \n",
    "    def pev_objective(selected_indices, data):\n",
    "        \"\"\"Custom PEV objective for user selection\"\"\"\n",
    "        K = data[\"G\"]\n",
    "        lambda_val = data[\"lambda\"]\n",
    "        n_total = K.shape[0]\n",
    "        \n",
    "        # Target users are all non-selected users\n",
    "        target_indices = [i for i in range(n_total) if i not in selected_indices]\n",
    "        \n",
    "        if len(target_indices) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate PEV for target users\n",
    "        K_TT = K[np.ix_(selected_indices, selected_indices)]\n",
    "        K_PT = K[np.ix_(target_indices, selected_indices)]\n",
    "        K_PP_diag = np.diag(K)[target_indices]\n",
    "        \n",
    "        # Add lambda to diagonal for numerical stability\n",
    "        K_TT_inv = np.linalg.inv(K_TT + lambda_val * np.eye(len(selected_indices)))\n",
    "        \n",
    "        # Calculate PEV for each target user\n",
    "        pev_values = []\n",
    "        for i, target_idx in enumerate(target_indices):\n",
    "            k_i = K_PT[i:i+1, :]  # Row vector\n",
    "            pev_i = K_PP_diag[i] - k_i @ K_TT_inv @ k_i.T\n",
    "            pev_values.append(pev_i[0, 0])\n",
    "        \n",
    "        # Return negative mean PEV (since we want to minimize PEV)\n",
    "        mean_pev = np.mean(pev_values)\n",
    "        return -mean_pev\n",
    "    \n",
    "    # Prepare data\n",
    "    ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "    \n",
    "    # Set up optimization\n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 100\n",
    "    control[\"npop\"] = 200\n",
    "    control[\"progress\"] = True\n",
    "    \n",
    "    print(f\"Optimizing selection of {n_select} users using PEV criterion...\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=pev_objective,\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    selected_users = result.selected_indices[0]\n",
    "    \n",
    "    print(f\"\\nPEV optimization completed!\")\n",
    "    print(f\"Final objective value: {result.fitness:.6f}\")\n",
    "    print(f\"Selected users: {selected_users[:10]}... (showing first 10)\")\n",
    "    \n",
    "    return selected_users, -result.fitness  # Convert back to positive PEV\n",
    "\n",
    "# Run PEV optimization\n",
    "optimal_users_pev, pev_value = optimize_user_selection_pev(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Comparison with Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_selection_methods():\n",
    "    \"\"\"\n",
    "    Compare optimal selection methods with random selection\n",
    "    \"\"\"\n",
    "    n_select = 100\n",
    "    n_random_trials = 50\n",
    "    \n",
    "    # Random selection baselines\n",
    "    random_cdmeans = []\n",
    "    random_pevs = []\n",
    "    \n",
    "    print(\"Running random selection trials...\")\n",
    "    for trial in range(n_random_trials):\n",
    "        random_users = np.random.choice(len(user_data), size=n_select, replace=False)\n",
    "        \n",
    "        # Calculate CDMean for random selection\n",
    "        ts_data_random = {\"G\": K_users, \"lambda\": 0.1}\n",
    "        cdmean_random = cdmean_opt(random_users.tolist(), ts_data_random)\n",
    "        random_cdmeans.append(cdmean_random)\n",
    "        \n",
    "        # Calculate mean PEV for random selection\n",
    "        non_selected = [i for i in range(len(user_data)) if i not in random_users]\n",
    "        K_TT = K_users[np.ix_(random_users, random_users)]\n",
    "        K_PT = K_users[np.ix_(non_selected, random_users)]\n",
    "        K_PP_diag = np.diag(K_users)[non_selected]\n",
    "        \n",
    "        K_TT_inv = np.linalg.inv(K_TT + 0.1 * np.eye(len(random_users)))\n",
    "        \n",
    "        pev_values = []\n",
    "        for i in range(len(non_selected)):\n",
    "            k_i = K_PT[i:i+1, :]\n",
    "            pev_i = K_PP_diag[i] - k_i @ K_TT_inv @ k_i.T\n",
    "            pev_values.append(pev_i[0, 0])\n",
    "        \n",
    "        random_pevs.append(np.mean(pev_values))\n",
    "    \n",
    "    # Create comparison results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Method': ['Random (mean)', 'Random (std)', 'CDMean Optimal', 'PEV Optimal'],\n",
    "        'CDMean': [\n",
    "            np.mean(random_cdmeans),\n",
    "            np.std(random_cdmeans),\n",
    "            cdmean_value,\n",
    "            np.nan  # We didn't calculate CDMean for PEV optimal\n",
    "        ],\n",
    "        'Mean PEV': [\n",
    "            np.mean(random_pevs),\n",
    "            np.std(random_pevs),\n",
    "            np.nan,  # We didn't calculate PEV for CDMean optimal\n",
    "            pev_value\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    cdmean_improvement = (cdmean_value - np.mean(random_cdmeans)) / np.mean(random_cdmeans) * 100\n",
    "    pev_improvement = (np.mean(random_pevs) - pev_value) / np.mean(random_pevs) * 100\n",
    "    \n",
    "    print(f\"\\nImprovements over random selection:\")\n",
    "    print(f\"  - CDMean optimization: {cdmean_improvement:.2f}% improvement\")\n",
    "    print(f\"  - PEV optimization: {pev_improvement:.2f}% improvement (lower is better)\")\n",
    "    \n",
    "    return results_df, random_cdmeans, random_pevs\n",
    "\n",
    "# Run comparison\n",
    "comparison_results, random_cdmeans, random_pevs = compare_selection_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_selection_results():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of selection results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. User characteristics comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Get characteristics for different selection methods\n",
    "    random_sample = np.random.choice(len(user_data), size=100, replace=False)\n",
    "    \n",
    "    methods = ['Random', 'CDMean Optimal', 'PEV Optimal']\n",
    "    selections = [random_sample, optimal_users_cdmean, optimal_users_pev]\n",
    "    colors = ['gray', 'blue', 'red']\n",
    "    \n",
    "    for i, (method, selection, color) in enumerate(zip(methods, selections, colors)):\n",
    "        selected_ages = user_data.iloc[selection]['age']\n",
    "        ax1.hist(selected_ages, alpha=0.6, label=method, color=color, bins=20)\n",
    "    \n",
    "    ax1.set_xlabel('Age')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Age Distribution by Selection Method')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Income distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    for i, (method, selection, color) in enumerate(zip(methods, selections, colors)):\n",
    "        selected_income = user_data.iloc[selection]['income']\n",
    "        ax2.hist(selected_income, alpha=0.6, label=method, color=color, bins=20)\n",
    "    \n",
    "    ax2.set_xlabel('Income')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Income Distribution by Selection Method')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. CDMean comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.hist(random_cdmeans, alpha=0.7, color='gray', bins=20, label='Random Selection')\n",
    "    ax3.axvline(cdmean_value, color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'CDMean Optimal: {cdmean_value:.6f}')\n",
    "    ax3.set_xlabel('CDMean Value')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('CDMean Distribution')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. PEV comparison\n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.hist(random_pevs, alpha=0.7, color='gray', bins=20, label='Random Selection')\n",
    "    ax4.axvline(pev_value, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'PEV Optimal: {pev_value:.6f}')\n",
    "    ax4.set_xlabel('Mean PEV Value')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Mean PEV Distribution')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Feature importance for optimal selections\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Calculate mean feature values for each selection method\n",
    "    feature_names = ['Age', 'Income', 'Sessions/Month', 'Session Duration', \n",
    "                     'Purchases', 'Order Value', 'Mobile', 'Premium']\n",
    "    feature_cols_viz = ['age', 'income', 'sessions_per_month', 'avg_session_duration',\n",
    "                        'purchases_last_year', 'avg_order_value', 'is_mobile', 'is_premium_user']\n",
    "    \n",
    "    # Normalize features for comparison\n",
    "    scaler = StandardScaler()\n",
    "    all_features_norm = scaler.fit_transform(user_data[feature_cols_viz])\n",
    "    \n",
    "    random_means = np.mean(all_features_norm[random_sample], axis=0)\n",
    "    cdmean_means = np.mean(all_features_norm[optimal_users_cdmean], axis=0)\n",
    "    pev_means = np.mean(all_features_norm[optimal_users_pev], axis=0)\n",
    "    \n",
    "    x = np.arange(len(feature_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax5.bar(x - width, random_means, width, label='Random', color='gray', alpha=0.7)\n",
    "    ax5.bar(x, cdmean_means, width, label='CDMean Optimal', color='blue', alpha=0.7)\n",
    "    ax5.bar(x + width, pev_means, width, label='PEV Optimal', color='red', alpha=0.7)\n",
    "    \n",
    "    ax5.set_xlabel('Features')\n",
    "    ax5.set_ylabel('Normalized Mean Value')\n",
    "    ax5.set_title('Feature Profiles by Selection Method')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(feature_names, rotation=45)\n",
    "    ax5.legend()\n",
    "    \n",
    "    # 6. User similarity heatmap\n",
    "    ax6 = axes[1, 2]\n",
    "    \n",
    "    # Sample small subset for visualization\n",
    "    viz_sample = np.random.choice(len(user_data), size=50, replace=False)\n",
    "    K_viz = K_users[np.ix_(viz_sample, viz_sample)]\n",
    "    \n",
    "    im = ax6.imshow(K_viz, cmap='viridis', aspect='auto')\n",
    "    ax6.set_title('User Similarity Matrix (Sample)')\n",
    "    ax6.set_xlabel('User Index')\n",
    "    ax6.set_ylabel('User Index')\n",
    "    plt.colorbar(im, ax=ax6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Random selection CDMean: {np.mean(random_cdmeans):.6f} ± {np.std(random_cdmeans):.6f}\")\n",
    "    print(f\"Optimal CDMean: {cdmean_value:.6f} (improvement: {((cdmean_value - np.mean(random_cdmeans)) / np.mean(random_cdmeans) * 100):.2f}%)\")\n",
    "    print(f\"Random selection PEV: {np.mean(random_pevs):.6f} ± {np.std(random_pevs):.6f}\")\n",
    "    print(f\"Optimal PEV: {pev_value:.6f} (improvement: {((np.mean(random_pevs) - pev_value) / np.mean(random_pevs) * 100):.2f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "visualize_selection_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Applications\n",
    "\n",
    "### 6.1 Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_objective_user_selection(n_select=100):\n",
    "    \"\"\"\n",
    "    Multi-objective optimization balancing CDMean, cost, and diversity\n",
    "    \"\"\"\n",
    "    def multi_objective_fitness(selected_indices, data):\n",
    "        \"\"\"\n",
    "        Combine multiple objectives:\n",
    "        1. CDMean (prediction accuracy)\n",
    "        2. Cost (based on user characteristics)\n",
    "        3. Diversity (demographic balance)\n",
    "        \"\"\"\n",
    "        # Objective 1: CDMean\n",
    "        cdmean_val = cdmean_opt(selected_indices, data)\n",
    "        \n",
    "        # Objective 2: Cost (higher income users are more expensive to recruit)\n",
    "        selected_users_df = user_data.iloc[selected_indices]\n",
    "        avg_income = selected_users_df['income'].mean()\n",
    "        cost_obj = -avg_income / 100000  # Negative because we want to minimize cost\n",
    "        \n",
    "        # Objective 3: Diversity (variance in key demographics)\n",
    "        age_diversity = selected_users_df['age'].std() / user_data['age'].std()\n",
    "        income_diversity = selected_users_df['income'].std() / user_data['income'].std()\n",
    "        diversity_obj = (age_diversity + income_diversity) / 2\n",
    "        \n",
    "        # Return multiple objectives\n",
    "        return [cdmean_val, cost_obj, diversity_obj]\n",
    "    \n",
    "    # Prepare data\n",
    "    ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "    \n",
    "    control = set_control_default()\n",
    "    control[\"niterations\"] = 50  # Reduced for multi-objective\n",
    "    control[\"npop\"] = 200\n",
    "    \n",
    "    print(\"Running multi-objective optimization...\")\n",
    "    print(\"Objectives: CDMean (max), Cost (min), Diversity (max)\")\n",
    "    \n",
    "    # Run multi-objective optimization\n",
    "    result = train_sel(\n",
    "        data=ts_data,\n",
    "        candidates=[list(range(len(user_data)))],\n",
    "        setsizes=[n_select],\n",
    "        settypes=[\"UOS\"],\n",
    "        stat=multi_objective_fitness,\n",
    "        n_stat=3,  # Three objectives\n",
    "        control=control,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMulti-objective optimization completed!\")\n",
    "    print(f\"Found {len(result.pareto_front)} solutions on Pareto front\")\n",
    "    \n",
    "    # Analyze Pareto front\n",
    "    if result.pareto_front:\n",
    "        pareto_df = pd.DataFrame(result.pareto_front, \n",
    "                                columns=['CDMean', 'Cost (neg)', 'Diversity'])\n",
    "        pareto_df['Cost'] = -pareto_df['Cost (neg)']  # Convert back to positive\n",
    "        \n",
    "        print(\"\\nPareto Front Solutions:\")\n",
    "        print(pareto_df.describe())\n",
    "        \n",
    "        # Select compromise solution (closest to ideal point)\n",
    "        # Normalize objectives to [0,1] range\n",
    "        normalized_objectives = (pareto_df[['CDMean', 'Diversity']] - pareto_df[['CDMean', 'Diversity']].min()) / (pareto_df[['CDMean', 'Diversity']].max() - pareto_df[['CDMean', 'Diversity']].min())\n",
    "        normalized_cost = (pareto_df['Cost'].max() - pareto_df['Cost']) / (pareto_df['Cost'].max() - pareto_df['Cost'].min())\n",
    "        \n",
    "        # Ideal point: maximize CDMean and Diversity, minimize Cost\n",
    "        ideal_point = np.array([1.0, 1.0, 1.0])  # All objectives at their best\n",
    "        \n",
    "        distances = []\n",
    "        for i in range(len(normalized_objectives)):\n",
    "            point = np.array([normalized_objectives.iloc[i]['CDMean'], \n",
    "                             normalized_cost.iloc[i], \n",
    "                             normalized_objectives.iloc[i]['Diversity']])\n",
    "            distance = np.linalg.norm(point - ideal_point)\n",
    "            distances.append(distance)\n",
    "        \n",
    "        best_compromise_idx = np.argmin(distances)\n",
    "        best_solution = result.pareto_solutions[best_compromise_idx]\n",
    "        \n",
    "        print(f\"\\nBest compromise solution (index {best_compromise_idx}):\")\n",
    "        print(f\"  - CDMean: {result.pareto_front[best_compromise_idx][0]:.6f}\")\n",
    "        print(f\"  - Cost: {-result.pareto_front[best_compromise_idx][1]:.0f}\")\n",
    "        print(f\"  - Diversity: {result.pareto_front[best_compromise_idx][2]:.4f}\")\n",
    "        \n",
    "        return result, best_solution['selected_indices'][0]\n",
    "    \n",
    "    return result, None\n",
    "\n",
    "# Run multi-objective optimization\n",
    "mo_result, best_compromise_users = multi_objective_user_selection(n_select=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Adaptive Experiment Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_experiment_design(initial_users=50, additional_batches=3, batch_size=25):\n",
    "    \"\"\"\n",
    "    Demonstrate adaptive experiment design where we sequentially add users\n",
    "    based on current results\n",
    "    \"\"\"\n",
    "    print(\"Adaptive Experiment Design Simulation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Start with initial random selection\n",
    "    current_users = np.random.choice(len(user_data), size=initial_users, replace=False).tolist()\n",
    "    \n",
    "    adaptation_history = []\n",
    "    \n",
    "    for batch in range(additional_batches):\n",
    "        print(f\"\\nBatch {batch + 1}: Adding {batch_size} more users\")\n",
    "        \n",
    "        # Calculate current CDMean\n",
    "        ts_data = {\"G\": K_users, \"lambda\": 0.1}\n",
    "        current_cdmean = cdmean_opt(current_users, ts_data)\n",
    "        \n",
    "        print(f\"Current CDMean with {len(current_users)} users: {current_cdmean:.6f}\")\n",
    "        \n",
    "        # Find remaining candidates\n",
    "        remaining_candidates = [i for i in range(len(user_data)) if i not in current_users]\n",
    "        \n",
    "        # Optimize selection of additional users\n",
    "        def incremental_fitness(additional_indices, data):\n",
    "            combined_users = current_users + additional_indices\n",
    "            return cdmean_opt(combined_users, data)\n",
    "        \n",
    "        control = set_control_default()\n",
    "        control[\"niterations\"] = 30\n",
    "        control[\"npop\"] = 100\n",
    "        \n",
    "        result = train_sel(\n",
    "            data=ts_data,\n",
    "            candidates=[remaining_candidates],\n",
    "            setsizes=[batch_size],\n",
    "            settypes=[\"UOS\"],\n",
    "            stat=incremental_fitness,\n",
    "            control=control,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Add selected users\n",
    "        new_users = result.selected_indices[0]\n",
    "        current_users.extend(new_users)\n",
    "        \n",
    "        new_cdmean = result.fitness\n",
    "        improvement = new_cdmean - current_cdmean\n",
    "        \n",
    "        print(f\"Added users: {new_users[:5]}... (showing first 5)\")\n",
    "        print(f\"New CDMean: {new_cdmean:.6f}\")\n",
    "        print(f\"Improvement: {improvement:.6f} ({improvement/current_cdmean*100:.2f}%)\")\n",
    "        \n",
    "        adaptation_history.append({\n",
    "            'batch': batch + 1,\n",
    "            'total_users': len(current_users),\n",
    "            'cdmean': new_cdmean,\n",
    "            'improvement': improvement,\n",
    "            'new_users': new_users\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nFinal adaptive selection:\")\n",
    "    print(f\"  - Total users: {len(current_users)}\")\n",
    "    print(f\"  - Final CDMean: {adaptation_history[-1]['cdmean']:.6f}\")\n",
    "    print(f\"  - Total improvement: {adaptation_history[-1]['cdmean'] - cdmean_opt(current_users[:initial_users], ts_data):.6f}\")\n",
    "    \n",
    "    return current_users, adaptation_history\n",
    "\n",
    "# Run adaptive design\n",
    "adaptive_users, adaptation_history = adaptive_experiment_design()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Business Impact Analysis\n",
    "\n",
    "### 7.1 Statistical Power Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis_comparison():\n",
    "    \"\"\"\n",
    "    Compare statistical power between optimal and random user selection\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Simulate A/B test results for different selection methods\n",
    "    methods = {\n",
    "        'Random': np.random.choice(len(user_data), size=100, replace=False),\n",
    "        'CDMean Optimal': optimal_users_cdmean,\n",
    "        'PEV Optimal': optimal_users_pev\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for method_name, selected_users in methods.items():\n",
    "        # Get outcomes for selected users\n",
    "        selected_outcomes = outcomes_df.iloc[selected_users]\n",
    "        \n",
    "        # Calculate treatment effect\n",
    "        control_rate = selected_outcomes['control_rate'].mean()\n",
    "        treatment_rate = selected_outcomes['treatment_rate'].mean()\n",
    "        observed_lift = (treatment_rate - control_rate) / control_rate\n",
    "        \n",
    "        # Statistical test\n",
    "        control_conversions = selected_outcomes['control_conversions'].sum()\n",
    "        control_sessions = selected_outcomes['n_sessions'].sum()\n",
    "        treatment_conversions = selected_outcomes['treatment_conversions'].sum()\n",
    "        treatment_sessions = selected_outcomes['n_sessions'].sum()\n",
    "        \n",
    "        # Two-proportion z-test\n",
    "        p_pooled = (control_conversions + treatment_conversions) / (control_sessions + treatment_sessions)\n",
    "        se_pooled = np.sqrt(p_pooled * (1 - p_pooled) * (1/control_sessions + 1/treatment_sessions))\n",
    "        z_stat = (treatment_rate - control_rate) / se_pooled\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        # Effect size (Cohen's h for proportions)\n",
    "        cohens_h = 2 * (np.arcsin(np.sqrt(treatment_rate)) - np.arcsin(np.sqrt(control_rate)))\n",
    "        \n",
    "        results.append({\n",
    "            'Method': method_name,\n",
    "            'Control Rate': control_rate,\n",
    "            'Treatment Rate': treatment_rate,\n",
    "            'Observed Lift': observed_lift,\n",
    "            'Z-statistic': z_stat,\n",
    "            'P-value': p_value,\n",
    "            'Significant (α=0.05)': p_value < 0.05,\n",
    "            'Cohen\\'s h': cohens_h,\n",
    "            'Sample Size': len(selected_users)\n",
    "        })\n",
    "    \n",
    "    power_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"Statistical Power Analysis:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(power_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Calculate required sample sizes for 80% power\n",
    "    print(\"\\nSample size requirements for 80% power:\")\n",
    "    alpha = 0.05\n",
    "    power = 0.80\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    for _, row in power_df.iterrows():\n",
    "        control_rate = row['Control Rate']\n",
    "        effect_size = row['Observed Lift']\n",
    "        treatment_rate = control_rate * (1 + effect_size)\n",
    "        \n",
    "        p_pooled = (control_rate + treatment_rate) / 2\n",
    "        required_n = ((z_alpha + z_beta) * np.sqrt(2 * p_pooled * (1 - p_pooled)) / (treatment_rate - control_rate))**2\n",
    "        \n",
    "        efficiency_gain = (required_n / row['Sample Size'] - 1) * 100 if row['Significant (α=0.05)'] else np.nan\n",
    "        \n",
    "        print(f\"  {row['Method']}: {required_n:.0f} users per group (efficiency gain: {efficiency_gain:.1f}%)\")\n",
    "    \n",
    "    return power_df\n",
    "\n",
    "# Run power analysis\n",
    "power_comparison = power_analysis_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Cost-Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_benefit_analysis():\n",
    "    \"\"\"\n",
    "    Estimate business value of optimal user selection\n",
    "    \"\"\"\n",
    "    # Assumptions\n",
    "    cost_per_user = 50  # Cost to include one user in experiment\n",
    "    revenue_per_conversion = 100  # Average revenue per conversion\n",
    "    total_traffic_per_month = 100000  # Total monthly users\n",
    "    \n",
    "    print(\"Cost-Benefit Analysis of Optimal User Selection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Random baseline\n",
    "    random_sample = np.random.choice(len(user_data), size=100, replace=False)\n",
    "    random_outcomes = outcomes_df.iloc[random_sample]\n",
    "    random_control_rate = random_outcomes['control_rate'].mean()\n",
    "    random_treatment_rate = random_outcomes['treatment_rate'].mean()\n",
    "    random_lift = (random_treatment_rate - random_control_rate) / random_control_rate\n",
    "    \n",
    "    # Optimal selection outcomes\n",
    "    optimal_outcomes = outcomes_df.iloc[optimal_users_cdmean]\n",
    "    optimal_control_rate = optimal_outcomes['control_rate'].mean()\n",
    "    optimal_treatment_rate = optimal_outcomes['treatment_rate'].mean()\n",
    "    optimal_lift = (optimal_treatment_rate - optimal_control_rate) / optimal_control_rate\n",
    "    \n",
    "    print(f\"Experiment costs:\")\n",
    "    print(f\"  - Cost per user in experiment: ${cost_per_user}\")\n",
    "    print(f\"  - Total experiment cost: ${100 * cost_per_user:,}\")\n",
    "    \n",
    "    print(f\"\\nObserved effects:\")\n",
    "    print(f\"  - Random selection lift: {random_lift:.3f} ({random_lift*100:.1f}%)\")\n",
    "    print(f\"  - Optimal selection lift: {optimal_lift:.3f} ({optimal_lift*100:.1f}%)\")\n",
    "    \n",
    "    # Assume the optimal selection gives a more accurate estimate of true population effect\n",
    "    # due to better representativeness (higher CDMean)\n",
    "    \n",
    "    # Simulate deployment to full population\n",
    "    monthly_conversions_control = total_traffic_per_month * random_control_rate\n",
    "    monthly_conversions_treatment_random = total_traffic_per_month * (random_control_rate * (1 + random_lift))\n",
    "    monthly_conversions_treatment_optimal = total_traffic_per_month * (random_control_rate * (1 + optimal_lift))\n",
    "    \n",
    "    # Revenue impact\n",
    "    monthly_revenue_baseline = monthly_conversions_control * revenue_per_conversion\n",
    "    monthly_revenue_improvement_random = (monthly_conversions_treatment_random - monthly_conversions_control) * revenue_per_conversion\n",
    "    monthly_revenue_improvement_optimal = (monthly_conversions_treatment_optimal - monthly_conversions_control) * revenue_per_conversion\n",
    "    \n",
    "    # The key insight: optimal selection provides more accurate estimate of true effect\n",
    "    # This reduces risk of deploying ineffective treatments or missing good ones\n",
    "    \n",
    "    accuracy_improvement = optimal_lift / random_lift if random_lift > 0 else 1\n",
    "    \n",
    "    print(f\"\\nProjected monthly impact (if treatment is deployed):\")\n",
    "    print(f\"  - Baseline monthly revenue: ${monthly_revenue_baseline:,.0f}\")\n",
    "    print(f\"  - Revenue improvement (random estimate): ${monthly_revenue_improvement_random:,.0f}\")\n",
    "    print(f\"  - Revenue improvement (optimal estimate): ${monthly_revenue_improvement_optimal:,.0f}\")\n",
    "    print(f\"  - Difference in estimates: ${abs(monthly_revenue_improvement_optimal - monthly_revenue_improvement_random):,.0f}\")\n",
    "    \n",
    "    # ROI calculation\n",
    "    experiment_cost = 100 * cost_per_user\n",
    "    monthly_value_difference = abs(monthly_revenue_improvement_optimal - monthly_revenue_improvement_random)\n",
    "    roi_months = experiment_cost / monthly_value_difference if monthly_value_difference > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\nReturn on Investment:\")\n",
    "    print(f\"  - Additional experiment cost for optimal selection: $0 (same sample size)\")\n",
    "    print(f\"  - Value of more accurate effect estimate: ${monthly_value_difference:,.0f}/month\")\n",
    "    print(f\"  - Break-even time: {roi_months:.1f} months\")\n",
    "    \n",
    "    # Risk reduction\n",
    "    print(f\"\\nRisk reduction benefits:\")\n",
    "    print(f\"  - Reduced risk of false positives/negatives\")\n",
    "    print(f\"  - Better external validity (CDMean: {cdmean_value:.4f} vs random mean: {np.mean(random_cdmeans):.4f})\")\n",
    "    print(f\"  - More confident business decisions\")\n",
    "    \n",
    "    return {\n",
    "        'experiment_cost': experiment_cost,\n",
    "        'monthly_value_difference': monthly_value_difference,\n",
    "        'roi_months': roi_months,\n",
    "        'accuracy_improvement': accuracy_improvement\n",
    "    }\n",
    "\n",
    "# Run cost-benefit analysis\n",
    "cost_benefit_results = cost_benefit_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Future Directions\n",
    "\n",
    "### 8.1 Key Findings\n",
    "\n",
    "Our analysis demonstrates several critical advantages of using TrainSelPy for A/B test design:\n",
    "\n",
    "1. **Improved Prediction Accuracy**: CDMean optimization achieved X% improvement over random selection\n",
    "2. **Reduced Prediction Uncertainty**: PEV optimization reduced prediction variance by Y%\n",
    "3. **Enhanced External Validity**: Optimal selection provides better generalization to target populations\n",
    "4. **Multi-objective Flexibility**: Can balance statistical power, cost, and diversity simultaneously\n",
    "\n",
    "### 8.2 Practical Implementation Guidelines\n",
    "\n",
    "1. **When to Use Optimal Selection**:\n",
    "   - Limited testing capacity (< 10% of user base)\n",
    "   - High-stakes experiments requiring maximum precision\n",
    "   - Need for strong external validity guarantees\n",
    "   - Multi-constraint optimization problems\n",
    "\n",
    "2. **Recommended Workflow**:\n",
    "   - Collect user characteristics and compute similarity matrix\n",
    "   - Define optimization objectives (CDMean, PEV, cost, diversity)\n",
    "   - Run TrainSelPy optimization with appropriate constraints\n",
    "   - Validate selection with pilot studies\n",
    "   - Deploy optimized experiment design\n",
    "\n",
    "3. **Key Considerations**:\n",
    "   - Quality of user similarity matrix is crucial\n",
    "   - Variance ratio (λ) estimation affects results\n",
    "   - Computational cost scales with problem size\n",
    "   - Results should be validated against business outcomes\n",
    "\n",
    "### 8.3 Future Research Directions\n",
    "\n",
    "1. **Dynamic Similarity Matrices**: Update user relationships based on real-time behavior\n",
    "2. **Causal Inference Integration**: Combine with doubly-robust estimation methods\n",
    "3. **Network Effect Modeling**: Extend to experiments with user interactions\n",
    "4. **Bayesian Optimization**: Incorporate prior beliefs about treatment effects\n",
    "5. **Multi-Armed Bandit Hybrid**: Combine optimal design with adaptive allocation\n",
    "\n",
    "### 8.4 Business Impact Summary\n",
    "\n",
    "The integration of mixed models and genetic algorithm optimization represents a paradigm shift from traditional A/B testing:\n",
    "\n",
    "- **From random sampling** → **strategic participant selection**\n",
    "- **From single objectives** → **multi-objective optimization**\n",
    "- **From fixed designs** → **adaptive experimentation**\n",
    "- **From internal validity focus** → **external validity optimization**\n",
    "\n",
    "This approach is particularly valuable for organizations with:\n",
    "- Large user bases requiring sample efficiency\n",
    "- Complex user heterogeneity patterns\n",
    "- Multiple competing business objectives\n",
    "- Strong needs for generalizability\n",
    "\n",
    "The mathematical rigor of mixed models combined with the optimization power of genetic algorithms opens new frontiers in experimental design, making A/B testing more scientific, efficient, and business-aligned.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Henderson, C.R. (1984). Applications of linear models in animal breeding. University of Guelph.\n",
    "2. Akdemir, D., et al. (2021). TrainSel: An R Package for Selection of Training Populations. *Frontiers in Genetics*, 12, 655287.\n",
    "3. Deng, A., et al. (2013). Improving the sensitivity of online controlled experiments by utilizing pre-experiment data. *Proceedings of WSDM*.\n",
    "4. Kohavi, R., et al. (2020). Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n",
    "5. Athey, S., & Imbens, G. W. (2017). The econometrics of randomized experiments. *Handbook of Economic Field Experiments*, 1, 73-140."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
